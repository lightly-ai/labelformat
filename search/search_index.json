{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":""},{"location":"#labelformat-fast-label-conversion-for-computer-vision","title":"Labelformat - Fast Label Conversion for Computer Vision","text":"<p>Labelformat is an open-source Python framework for converting between popular computer vision annotation formats like YOLO, COCO, PascalVOC, and KITTI. Save hours on tedious format conversions and ensure consistency in your workflows.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Wide Format Support: COCO, YOLO (v5-v12, v26), PascalVOC, KITTI, Labelbox, RT-DETR, RT-DETRv2, and more.</li> <li>Cross-Platform: Compatible with Python 3.8+ on Windows, macOS, and Linux.</li> <li>Flexible Usage: Intuitive CLI and Python API.</li> <li>Efficient: Memory-conscious, optimized for large datasets.</li> <li>Offline First: Operates locally without data uploads.</li> <li>Tested for Accuracy: Round-trip tests for consistent results.</li> </ul>"},{"location":"#get-started-quickly","title":"Get Started Quickly","text":"<ol> <li>Install via pip:     <pre><code>pip install labelformat\n</code></pre></li> <li>Convert Labels in One Command:     <pre><code>labelformat convert --task object-detection \\\n                    --input-format coco \\\n                    --input-file coco-labels/train.json \\\n                    --output-format yolov8 \\\n                    --output-file yolo-labels/data.yaml\n</code></pre></li> </ol>"},{"location":"#supported-formats","title":"Supported Formats","text":""},{"location":"#2d-object-detection-label-formats","title":"2D Object Detection Label Formats","text":"Format Read \u2714\ufe0f Write \u2714\ufe0f COCO \u2714\ufe0f \u2714\ufe0f KITTI \u2714\ufe0f \u2714\ufe0f Labelbox \u2714\ufe0f \u274c Lightly \u2714\ufe0f \u2714\ufe0f PascalVOC \u2714\ufe0f \u2714\ufe0f RT-DETR \u2714\ufe0f \u2714\ufe0f RT-DETRv2 \u2714\ufe0f \u2714\ufe0f YOLOv5 - v12, v26 \u2714\ufe0f \u2714\ufe0f"},{"location":"#2d-instance-segmentation-label-formats","title":"2D Instance Segmentation Label Formats","text":"Format Read \u2714\ufe0f Write \u2714\ufe0f COCO \u2714\ufe0f \u2714\ufe0f YOLOv8 \u2714\ufe0f \u2714\ufe0f"},{"location":"#explore-more","title":"Explore More","text":"<ul> <li>Quick Start Guide</li> <li>Detailed Usage Guide</li> <li>List of all features</li> </ul>"},{"location":"#quick-links","title":"\ud83d\udce6 Quick Links","text":"<ul> <li>GitHub Repository</li> <li>PyPI Package</li> <li>Documentation</li> </ul> <p>Labelformat is maintained by Lightly.</p>"},{"location":"about-us/","title":"About Us","text":"<p>Labelformat is maintained by Lightly, a spin-off from ETH Zurich dedicated to building efficient active learning pipelines for machine learning models. Our mission is to empower data scientists and engineers with tools that streamline data processing and model training workflows.</p>"},{"location":"about-us/#our-mission","title":"Our Mission","text":"<p>At Lightly, we aim to simplify the complexities of active learning and data management, enabling teams to focus on developing cutting-edge machine learning models without getting bogged down by data preparation challenges.</p>"},{"location":"about-us/#what-we-offer","title":"What We Offer","text":"<ul> <li>Active Learning Pipelines: Intelligent data selection to enhance model performance with minimal data.</li> <li>Efficient Data Management: Tools and services that optimize data workflows for scalability and efficiency.</li> <li>Expert Support: Dedicated support to help you integrate our solutions seamlessly into your projects.</li> </ul>"},{"location":"about-us/#learn-more","title":"Learn More","text":"<ul> <li>Homepage</li> <li>Web-App</li> <li>Lightly Solution Documentation</li> <li>Contact Us</li> </ul>"},{"location":"about-us/#connect-with-us","title":"Connect with Us","text":"<p>Stay updated with the latest developments, tips, and tutorials by following us:</p> <ul> <li>GitHub</li> <li>Twitter</li> </ul> <p>Labelformat is part of Lightly's commitment to fostering an open-source ecosystem that benefits the global machine learning community. Join us in making data management and label conversion effortless!</p>"},{"location":"features/","title":"Features","text":"<p>Labelformat offers a robust set of features tailored to meet the diverse needs of computer vision engineers and data scientists.</p>"},{"location":"features/#key-features","title":"Key Features","text":"<ul> <li> <p>Wide Format Support:</p> <ul> <li>2D Object Detection: Bounding box annotations for object localization</li> <li>Instance Segmentation: Pixel-level masks for precise object delineation</li> </ul> </li> <li> <p>User-Friendly CLI and Python API:</p> <ul> <li>CLI: Simple terminal commands to convert formats with customizable options.</li> <li>Python API: Integrate label conversion seamlessly into your Python workflows.</li> </ul> </li> <li> <p>Performance Optimizations:</p> <ul> <li>Memory Conscious: Processes datasets file-by-file to minimize memory usage.</li> <li>Minimal Dependencies: Targets Python 3.8 or higher, ensuring broad compatibility.</li> </ul> </li> <li> <p>Cross-Platform Support:</p> <ul> <li>Windows, Linux, and macOS: Works seamlessly across all major operating systems.</li> </ul> </li> <li> <p>Reliability and Testing:</p> <ul> <li>Typed Codebase: Ensures type safety and easier maintenance.</li> <li>Round-Trip Tests: Guarantees label consistency across conversions.</li> </ul> </li> <li> <p>Open-Source and Community-Driven:</p> <ul> <li>MIT License: Free to use and modify.</li> <li>Active Contributions: Regular updates and community support.</li> </ul> </li> </ul>"},{"location":"features/#supported-tasks-and-formats","title":"Supported Tasks and Formats","text":""},{"location":"features/#object-detection","title":"Object Detection","text":"<ul> <li>COCO</li> <li>KITTI</li> <li>Labelbox (input only)</li> <li>Lightly</li> <li>PascalVOC</li> <li>RT-DETR</li> <li>RT-DETRv2</li> <li>YOLOv5</li> <li>YOLOv6</li> <li>YOLOv7</li> <li>YOLOv8</li> <li>YOLOv9</li> <li>YOLOv10</li> <li>YOLOv11</li> <li>YOLOv12</li> <li>YOLOv26</li> </ul>"},{"location":"features/#why-labelformat","title":"Why Labelformat?","text":"<p>Labelformat addresses the common challenges faced when dealing with diverse label formats:</p> <ul> <li>Consistency: Ensures uniformity across different formats, crucial for model training.</li> <li>Efficiency: Reduces the time spent on manual label format conversions.</li> <li>Scalability: Handles large datasets with minimal memory footprint.</li> <li>Flexibility: Supports a growing list of formats and tasks, adapting to evolving project needs.</li> </ul> <p>Explore our Quick Start Guide to begin leveraging Labelformat's powerful features today!</p>"},{"location":"installation/","title":"Installation","text":"<p>Installing Labelformat is straightforward. Follow the steps below to set up Labelformat in your development environment.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher: Ensure you have Python installed on Windows, Linux, or macOS.</li> <li>pip: Python's package installer. It typically comes with Python installations.</li> </ul>"},{"location":"installation/#installation-using-package-managers","title":"Installation using package managers","text":"<p>Labelformat is available on PyPI and can be installed using various package managers:</p> pipPoetryCondaRye <pre><code>pip install labelformat\n</code></pre> <pre><code>poetry add labelformat\n</code></pre> <pre><code>conda install -c conda-forge labelformat\n</code></pre> <pre><code>rye add labelformat\n</code></pre>"},{"location":"installation/#installation-from-source","title":"Installation from Source","text":"<p>If you prefer to install Labelformat from the source code, follow these steps:</p> <ol> <li>Clone the Repository:    <pre><code>git clone https://github.com/lightly-ai/labelformat.git\ncd labelformat\n</code></pre></li> <li>Install Dependencies:    Labelformat uses Poetry for dependency management. Ensure you have Poetry installed:    <pre><code>pip install poetry\n</code></pre></li> <li>Set Up the Development Environment:    <pre><code>poetry install\n</code></pre></li> </ol>"},{"location":"installation/#updating-labelformat","title":"Updating Labelformat","text":"<p>To update Labelformat to the latest version, run: <pre><code>pip install --upgrade labelformat\n</code></pre></p>"},{"location":"quick-start/","title":"Quick Start Guide","text":"<p>Get up and running with Labelformat in minutes! This Quick Start Guide provides simple, copy-paste examples to help you convert label formats effortlessly.</p>"},{"location":"quick-start/#scenario-1-convert-coco-to-yolov8-using-cli","title":"Scenario 1: Convert COCO to YOLOv8 Using CLI","text":""},{"location":"quick-start/#step-1-prepare-your-files","title":"Step 1: Prepare Your Files","text":"<p>Ensure you have the following structure: <pre><code>project/\n\u251c\u2500\u2500 coco-labels/\n\u2502   \u2514\u2500\u2500 train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2514\u2500\u2500 image2.jpg\n</code></pre></p>"},{"location":"quick-start/#step-2-run-the-conversion-command","title":"Step 2: Run the Conversion Command","text":"<p>Open your terminal, navigate to your project directory, and execute:</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file coco-labels/train.json \\\n    --output-format yolov8 \\\n    --output-file yolo-labels/data.yaml \\\n    --output-split train\n</code></pre>"},{"location":"quick-start/#step-3-verify-the-output","title":"Step 3: Verify the Output","text":"<p>Your project structure should now include:</p> <pre><code>project/\n\u251c\u2500\u2500 yolo-labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n</code></pre>"},{"location":"quick-start/#scenario-2-convert-yolov8-to-coco-using-python-api","title":"Scenario 2: Convert YOLOv8 to COCO Using Python API","text":""},{"location":"quick-start/#step-1-install-labelformat","title":"Step 1: Install Labelformat","text":"<p>If you haven't installed Labelformat yet, do so via pip: <pre><code>pip install labelformat\n</code></pre></p>"},{"location":"quick-start/#step-2-write-the-conversion-script","title":"Step 2: Write the Conversion Script","text":"<p>Create a Python script, <code>convert_yolo_to_coco.py</code>, with the following content:</p> <pre><code>from pathlib import Path\nfrom labelformat.formats import COCOObjectDetectionOutput, YOLOv8ObjectDetectionInput\n\n# Load YOLOv8 labels\nyolo_input = YOLOv8ObjectDetectionInput(\n    input_file=Path(\"yolo-labels/data.yaml\"),\n    input_split=\"train\"\n)\n\n# Convert to COCO format and save\ncoco_output = COCOObjectDetectionOutput(\n    output_file=Path(\"coco-from-yolo/converted_coco.json\")\n)\ncoco_output.save(label_input=yolo_input)\n\nprint(\"Conversion from YOLOv8 to COCO completed successfully!\")\n</code></pre>"},{"location":"quick-start/#step-3-execute-the-script","title":"Step 3: Execute the Script","text":"<p>Run the script:</p> <pre><code>python convert_yolo_to_coco.py\n</code></pre>"},{"location":"quick-start/#step-4-check-the-coco-output","title":"Step 4: Check the COCO Output","text":"<p>Your project should now have:</p> <pre><code>project/\n\u251c\u2500\u2500 coco-from-yolo/\n\u2502   \u2514\u2500\u2500 converted_coco.json\n</code></pre>"},{"location":"quick-start/#scenario-3-convert-labelbox-export-to-lightly-format","title":"Scenario 3: Convert Labelbox Export to Lightly Format","text":""},{"location":"quick-start/#step-1-export-labels-from-labelbox","title":"Step 1: Export Labels from Labelbox","text":"<p>Ensure you have the Labelbox export file, e.g., <code>labelbox-export.ndjson</code>.</p>"},{"location":"quick-start/#step-2-run-the-conversion-command_1","title":"Step 2: Run the Conversion Command","text":"<pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format labelbox \\\n    --input-file labelbox-export.ndjson \\\n    --category-names cat,dog,fish \\\n    --output-format lightly \\\n    --output-folder lightly-labels/annotation-task\n</code></pre>"},{"location":"quick-start/#step-3-verify-the-lightly-output","title":"Step 3: Verify the Lightly Output","text":"<p>Your project structure should include:</p> <pre><code>project/\n\u251c\u2500\u2500 lightly-labels/\n\u2502   \u251c\u2500\u2500 annotation-task/\n\u2502   \u2502   \u251c\u2500\u2500 schema.json\n\u2502   \u2502   \u251c\u2500\u2500 image1.json\n\u2502   \u2502   \u2514\u2500\u2500 image2.json\n</code></pre>"},{"location":"usage/","title":"Detailed Usage Guide","text":"<p>Labelformat offers both a Command-Line Interface (CLI) and a Python API to cater to different workflows. This guide provides in-depth instructions on how to use both interfaces effectively.</p> <p>To get a detailed overview of the supported formats and their specifications, please refer to the Supported Object Detection Formats section.</p>"},{"location":"usage/#cli-usage","title":"CLI Usage","text":"<p>Labelformat's CLI provides a straightforward way to convert label formats directly from the terminal.</p>"},{"location":"usage/#basic-conversion-command","title":"Basic Conversion Command","text":"<p>Example: Convert Object Detection labels from COCO to YOLOv8.</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file path/to/coco/train.json \\\n    --output-format yolov8 \\\n    --output-file path/to/yolo/data.yaml \\\n    --output-split train\n</code></pre> <p>Parameters:</p> <ul> <li><code>--task</code>: Specify the task type (<code>object-detection</code> or <code>instance-segmentation</code>).</li> <li><code>--input-format</code>: The format of the input labels (e.g., <code>coco</code>).</li> <li><code>--input-file</code> or <code>--input-folder</code>: Path to the input label file or folder.</li> <li><code>--output-format</code>: The desired output label format (e.g., <code>yolov8</code>).</li> <li><code>--output-file</code> or <code>--output-folder</code>: Path to save the converted labels.</li> <li><code>--output-split</code>: Define the data split (<code>train</code>, <code>val</code>, <code>test</code>).</li> </ul>"},{"location":"usage/#advanced-cli-options","title":"Advanced CLI Options","text":"<p>Listing Supported Formats:</p> <p>To see all supported input and output formats for a specific task:</p> <pre><code>labelformat convert --task object-detection --help\n</code></pre> <p>Specifying Category Names:</p> <p>Some formats require explicit category names. The names must be separated by commas and must be in the same order as the categories in the input file.</p> <p>Use the <code>--category-names</code> argument to specify the category names:</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format labelbox \\\n    --input-file labelbox-export.ndjson \\\n    --category-names cat,dog,fish \\\n    --output-format coco \\\n    --output-file coco-output/train.json \\\n    --output-split train\n</code></pre> <p>Handling Missing Images:</p> <p>When converting formats that require image files (e.g., YOLO to COCO), ensure your image paths are correctly specified. Use <code>--images-rel-path</code> to define the relative path from the input folder to the images folder:</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format kitti \\\n    --input-folder kitti-labels/labels \\\n    --images-rel-path ../images \\\n    --output-format pascalvoc \\\n    --output-folder pascalvoc-labels\n</code></pre>"},{"location":"usage/#python-api-usage","title":"Python API Usage","text":"<p>For more flexible integrations, Labelformat provides a Python API.</p>"},{"location":"usage/#basic-conversion","title":"Basic Conversion","text":"<p>Example: Convert COCO to YOLOv8.</p> <pre><code>from pathlib import Path\nfrom labelformat.formats import COCOObjectDetectionInput, YOLOv8ObjectDetectionOutput\n\n# Initialize input and output classes\ncoco_input = COCOObjectDetectionInput(input_file=Path(\"coco-labels/train.json\"))\nyolo_output = YOLOv8ObjectDetectionOutput(\n    output_file=Path(\"yolo-labels/data.yaml\"),\n    output_split=\"train\"\n)\n\n# Perform the conversion\nyolo_output.save(label_input=coco_input)\n\nprint(\"Conversion from COCO to YOLOv8 completed successfully!\")\n</code></pre>"},{"location":"usage/#customizing-conversion","title":"Customizing Conversion","text":"<p>Example: Adding Custom Fields or Handling Special Cases.</p> <pre><code>from pathlib import Path\nfrom labelformat.formats import COCOInstanceSegmentationInput, YOLOv8InstanceSegmentationOutput\n\n# Initialize input for instance segmentation\ncoco_inst_input = COCOInstanceSegmentationInput(input_file=Path(\"coco-instance/train.json\"))\n\n# Initialize YOLOv8 instance segmentation output\nyolo_inst_output = YOLOv8InstanceSegmentationOutput(\n    output_file=Path(\"yolo-instance-labels/data.yaml\"),\n    output_split=\"train\"\n)\n\n# Perform the conversion\nyolo_inst_output.save(label_input=coco_inst_input)\n\nprint(\"Instance segmentation conversion completed successfully!\")\n</code></pre>"},{"location":"usage/#common-tasks","title":"Common Tasks","text":""},{"location":"usage/#handling-category-names","title":"Handling Category Names","text":"<p>Some label formats require you to specify category names explicitly. Ensure that category names are consistent across your dataset.</p> <p>Example:</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format labelbox \\\n    --input-file labelbox-export.ndjson \\\n    --category-names cat,dog,fish \\\n    --output-format coco \\\n    --output-file coco-output/train.json \\\n    --output-split train\n</code></pre>"},{"location":"usage/#managing-image-paths","title":"Managing Image Paths","text":"<p>When converting formats that reference image files, accurately specify the relative paths to avoid missing files.</p> <p>Example:</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format kitti \\\n    --input-folder kitti-labels/labels \\\n    --images-rel-path ../images \\\n    --output-format pascalvoc \\\n    --output-folder pascalvoc-labels\n</code></pre>"},{"location":"usage/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ul> <li>Backup Your Data: Always keep a backup of your original labels before performing conversions.</li> <li>Validate Output: After conversion, verify the output labels to ensure accuracy.</li> <li>Consistent Naming: Maintain consistent naming conventions for categories and files across different formats.</li> <li>Leverage Round-Trip Tests: Use Labelformat's testing capabilities to ensure label consistency when converting back and forth between formats.</li> </ul> <p>For more detailed examples and advanced usage scenarios, explore our Tutorials section.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"formats/","title":"Supported Formats","text":""},{"location":"formats/#object-detection","title":"Object Detection","text":"<ul> <li>COCO</li> <li>KITTI</li> <li>Labelbox</li> <li>Lightly</li> <li>PascalVOC</li> <li>RT-DETR</li> <li>RT-DETRv2</li> <li>YOLOv5</li> <li>YOLOv6</li> <li>YOLOv7</li> <li>YOLOv8</li> <li>YOLOv9</li> <li>YOLOv10</li> <li>YOLOv11</li> <li>YOLOv12</li> <li>YOLOv26</li> </ul>"},{"location":"formats/object-detection/","title":"Object Detection Overview","text":"<p>Object detection is a computer vision task that involves identifying and locating objects within images using rectangular bounding boxes. Each detection includes:</p> <ul> <li>A category label (e.g., \"car\", \"person\", \"dog\")</li> <li>A bounding box defining the object's location and size</li> <li>Optional confidence score indicating detection certainty</li> </ul> <p>Labelformat supports converting between major object detection annotation formats like COCO, YOLO, and Pascal VOC while preserving the essential bounding box coordinates and category information.</p>"},{"location":"formats/object-detection/#supported-formats","title":"Supported Formats","text":"<ul> <li>COCO</li> <li>KITTI</li> <li>Labelbox</li> <li>Lightly</li> <li>PascalVOC</li> <li>RT-DETR</li> <li>RT-DETRv2</li> <li>YOLOv5</li> <li>YOLOv6</li> <li>YOLOv7</li> <li>YOLOv8</li> <li>YOLOv9</li> <li>YOLOv10</li> <li>YOLOv11</li> <li>YOLOv12</li> <li>YOLOv26</li> </ul>"},{"location":"formats/object-detection/coco/","title":"COCO Object Detection Format","text":""},{"location":"formats/object-detection/coco/#overview","title":"Overview","text":"<p>COCO (Common Objects in Context) is a large-scale object detection dataset format developed by Microsoft. The format has become one of the most widely adopted standards for object detection tasks. You can find the complete format specification in the official COCO documentation.</p>"},{"location":"formats/object-detection/coco/#specification-of-coco-detection-format","title":"Specification of COCO Detection Format","text":"<p>COCO uses a single JSON file containing all annotations. The format consists of three main components:</p> <ul> <li>Images: Defines metadata for each image in the dataset.</li> <li>Categories: Defines the object classes.</li> <li>Annotations: Defines object instances.</li> </ul>"},{"location":"formats/object-detection/coco/#images","title":"Images","text":"<p>Defines metadata for each image in the dataset: <pre><code>{\n  \"id\": 0,                    // Unique image ID\n  \"file_name\": \"image1.jpg\",  // Image filename\n  \"width\": 640,              // Image width in pixels\n  \"height\": 416              // Image height in pixels\n}\n</code></pre></p>"},{"location":"formats/object-detection/coco/#categories","title":"Categories","text":"<p>Defines the object classes: <pre><code>{\n  \"id\": 0,                    // Unique category ID\n  \"name\": \"cat\"              // Category name\n}\n</code></pre></p>"},{"location":"formats/object-detection/coco/#annotations","title":"Annotations","text":"<p>Defines object instances: <pre><code>{\n  \"image_id\": 0,              // Reference to image\n  \"category_id\": 2,           // Reference to category\n  \"bbox\": [540.0, 295.0, 23.0, 18.0]  // [x, y, width, height] in absolute pixels\n}\n</code></pre></p>"},{"location":"formats/object-detection/coco/#directory-structure-of-coco-dataset","title":"Directory Structure of COCO Dataset","text":"<pre><code>dataset/\n\u251c\u2500\u2500 images/                   # Image files\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2514\u2500\u2500 image2.jpg\n\u2514\u2500\u2500 annotations.json         # Single JSON file containing all annotations\n</code></pre>"},{"location":"formats/object-detection/coco/#converting-with-labelformat","title":"Converting with Labelformat","text":""},{"location":"formats/object-detection/coco/#command-line-interface","title":"Command Line Interface","text":"<p>Convert COCO format to YOLOv8: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file coco-labels/annotations.json \\\n    --output-format yolov8 \\\n    --output-file yolo-labels/data.yaml \\\n    --output-split train\n</code></pre></p> <p>Convert YOLOv8 format to COCO: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format yolov8 \\\n    --input-file yolo-labels/data.yaml \\\n    --input-split train \\\n    --output-format coco \\\n    --output-file coco-labels/annotations.json\n</code></pre></p>"},{"location":"formats/object-detection/coco/#python-api","title":"Python API","text":"<pre><code>from pathlib import Path\nfrom labelformat.formats import COCOObjectDetectionInput, YOLOv8ObjectDetectionOutput\n\n# Load COCO format\nlabel_input = COCOObjectDetectionInput(\n    input_file=Path(\"coco-labels/annotations.json\")\n)\n\n# Convert to YOLOv8 format\nYOLOv8ObjectDetectionOutput(\n    output_file=Path(\"yolo-labels/data.yaml\"),\n    output_split=\"train\",\n).save(label_input=label_input)\n</code></pre>"},{"location":"formats/object-detection/coco/#example","title":"Example","text":"<p>Complete annotations.json example: <pre><code>{\n  \"images\": [\n    {\n      \"id\": 0,\n      \"file_name\": \"image1.jpg\",\n      \"width\": 640,\n      \"height\": 416\n    }\n  ],\n  \"categories\": [\n    {\n      \"id\": 0,\n      \"name\": \"cat\"\n    }\n  ],\n  \"annotations\": [\n    {\n      \"image_id\": 0,\n      \"category_id\": 0,\n      \"bbox\": [540.0, 295.0, 23.0, 18.0]\n    }\n  ]\n}\n</code></pre></p>"},{"location":"formats/object-detection/kitti/","title":"KITTI Object Detection Format","text":""},{"location":"formats/object-detection/kitti/#overview","title":"Overview","text":"<p>The KITTI format was developed as part of the KITTI Vision Benchmark Suite, focusing on autonomous driving scenarios. This format is particularly well-suited for 3D object detection and tracking tasks. The complete format specification can be found in the KITTI development kit documentation.</p>"},{"location":"formats/object-detection/kitti/#specification-of-kitti-detection-format","title":"Specification of KITTI Detection Format","text":"<p>Each object is represented by 15 space-separated values:</p> <pre><code>#Values    Name      Description\n----------------------------------------------------------------------------\n   1    type         Object type (Car, Van, Truck, etc.)\n   1    truncated    Float 0-1 (truncated ratio)\n   1    occluded     Integer (0=visible, 1=partly occluded, 2=fully occluded)\n   1    alpha        Observation angle (-pi..pi)\n   4    bbox         2D bounding box (x1,y1,x2,y2) in pixels\n   3    dimensions   3D dimensions (height, width, length) in meters\n   3    location     3D location (x,y,z) in camera coordinates\n   1    rotation_y   Rotation around Y-axis in camera coordinates\n</code></pre>"},{"location":"formats/object-detection/kitti/#directory-structure-of-kitti-dataset","title":"Directory Structure of KITTI Dataset","text":"<pre><code>dataset/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 000000.png\n\u2502   \u2514\u2500\u2500 000001.png\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 000000.txt\n    \u2514\u2500\u2500 000001.txt\n</code></pre>"},{"location":"formats/object-detection/kitti/#label-format","title":"Label Format","text":"<pre><code># Example: 000000.txt\nCar -1 -1 -10 614 181 727 284 -1 -1 -1 -1000 -1000 -1000 -10\nPedestrian -1 -1 -10 123 456 789 012 -1 -1 -1 -1000 -1000 -1000 -10\n</code></pre> <p>Note: The filename of each label file must match its corresponding image file, with .txt extension.</p>"},{"location":"formats/object-detection/kitti/#annotation-format-conversion","title":"Annotation Format Conversion","text":""},{"location":"formats/object-detection/kitti/#using-cli","title":"Using CLI","text":"<p>Convert from YOLOv8 to KITTI format: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format yolov8 \\\n    --input-file yolo-labels/data.yaml \\\n    --input-split train \\\n    --output-format kitti \\\n    --output-folder kitti-labels\n</code></pre></p> <p>Convert from KITTI to YOLOv8 format: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format kitti \\\n    --input-folder kitti-labels \\\n    --category-names car,pedestrian,cyclist \\\n    --images-rel-path ../images \\\n    --output-format yolov8 \\\n    --output-file yolo-labels/data.yaml \\\n    --output-split train\n</code></pre></p>"},{"location":"formats/object-detection/kitti/#using-python","title":"Using Python","text":"<pre><code>from pathlib import Path\nfrom labelformat.formats import KittiObjectDetectionInput, YOLOv8ObjectDetectionOutput\n\n# Load KITTI labels\nlabel_input = KittiObjectDetectionInput(\n    input_folder=Path(\"kitti-labels\"),\n    category_names=\"car,pedestrian,cyclist\",\n    images_rel_path=\"../images\"\n)\n\n# Convert to YOLOv8 and save\nYOLOv8ObjectDetectionOutput(\n    output_file=Path(\"yolo-labels/data.yaml\"),\n    output_split=\"train\"\n).save(label_input=label_input)\n</code></pre>"},{"location":"formats/object-detection/kitti/#notes","title":"Notes","text":"<ul> <li>KITTI format uses absolute pixel coordinates (x1,y1,x2,y2) for bounding boxes</li> <li>Some fields like truncated, occluded, dimensions etc. are optional and can be set to -1 if unknown</li> <li>The category name (type) should match one of the predefined categories when converting </li> </ul>"},{"location":"formats/object-detection/labelbox/","title":"Labelbox Object Detection Format","text":""},{"location":"formats/object-detection/labelbox/#overview","title":"Overview","text":"<p>Labelbox uses NDJSON (Newline Delimited JSON) format for label exports, where each line represents a single image and its annotations. The format supports object detection through bounding boxes. While Labelformat currently supports Labelbox as an input-only format, you can find the complete format specification in the Labelbox documentation.</p>"},{"location":"formats/object-detection/labelbox/#specification-of-labelbox-detection-format","title":"Specification of Labelbox Detection Format","text":"<pre><code>dataset/\n\u2514\u2500\u2500 export-result.ndjson\n</code></pre> <p>Each line in the NDJSON file contains a complete JSON object with three main sections:</p> <ul> <li><code>data_row</code>: Contains image metadata (id, filename, external references)</li> <li><code>media_attributes</code>: Image dimensions</li> <li><code>projects</code>: Contains the actual annotations</li> </ul>"},{"location":"formats/object-detection/labelbox/#label-format","title":"Label Format","text":"<p>Each annotation line follows this structure: <pre><code>{\n  \"data_row\": {\n    \"id\": \"data_row_id\",\n    \"global_key\": \"image1.jpg\",\n    \"external_id\": \"image1.jpg\"\n  },\n  \"media_attributes\": {\n    \"width\": 640,\n    \"height\": 480\n  },\n  \"projects\": {\n    \"project_id\": {\n      \"labels\": [{\n        \"annotations\": {\n          \"objects\": [{\n            \"name\": \"cat\",\n            \"annotation_kind\": \"ImageBoundingBox\",\n            \"bounding_box\": {\n              \"top\": 100,\n              \"left\": 200,\n              \"width\": 50,\n              \"height\": 30\n            }\n          }]\n        }\n      }]\n    }\n  }\n}\n</code></pre></p>"},{"location":"formats/object-detection/labelbox/#converting-from-labelbox-format","title":"Converting from Labelbox Format","text":"<p>Labelbox format can be converted to other formats using labelformat. Here's an example converting to YOLOv8:</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format labelbox \\\n    --input-file labelbox-labels/export-result.ndjson \\\n    --category-names cat,dog,fish \\\n    --output-format yolov8 \\\n    --output-file yolo-labels/data.yaml \\\n    --output-split train\n</code></pre>"},{"location":"formats/object-detection/labelbox/#important-parameters","title":"Important Parameters","text":"<ul> <li><code>--category-names</code>: Required list of category names (comma-separated)</li> <li><code>--filename-key</code>: Which key to use as filename (options: global_key, external_id, id; default: global_key)</li> </ul>"},{"location":"formats/object-detection/labelbox/#format-details","title":"Format Details","text":""},{"location":"formats/object-detection/labelbox/#bounding-box-format","title":"Bounding Box Format","text":"<ul> <li>Uses absolute pixel coordinates</li> <li>Format: <code>{top, left, width, height}</code></li> <li>Origin: Top-left corner of the image</li> </ul>"},{"location":"formats/object-detection/labelbox/#limitations","title":"Limitations","text":"<ul> <li>Currently supports single project exports only</li> <li>Video annotations are not supported</li> <li>Only <code>ImageBoundingBox</code> annotation types are processed</li> </ul>"},{"location":"formats/object-detection/labelbox/#example","title":"Example","text":"<pre><code>{\"data_row\":{\"id\":\"ckz...\",\"global_key\":\"image1.jpg\",\"external_id\":\"img_1\"},\"media_attributes\":{\"width\":640,\"height\":480},\"projects\":{\"proj_123\":{\"labels\":[{\"annotations\":{\"objects\":[{\"name\":\"cat\",\"annotation_kind\":\"ImageBoundingBox\",\"bounding_box\":{\"top\":100,\"left\":200,\"width\":50,\"height\":30}}]}}]}}}\n{\"data_row\":{\"id\":\"ckz...\",\"global_key\":\"image2.jpg\",\"external_id\":\"img_2\"},\"media_attributes\":{\"width\":640,\"height\":480},\"projects\":{\"proj_123\":{\"labels\":[{\"annotations\":{\"objects\":[{\"name\":\"dog\",\"annotation_kind\":\"ImageBoundingBox\",\"bounding_box\":{\"top\":150,\"left\":300,\"width\":60,\"height\":40}}]}}]}}}\n</code></pre> <p>Note: This format is supported for input only in labelformat. </p>"},{"location":"formats/object-detection/labelformat/","title":"Labelformat Object Detection Format","text":""},{"location":"formats/object-detection/labelformat/#overview","title":"Overview","text":"<p>The Labelformat format provides an in-memory representation for object detection detections. It's designed for programmatic creation and manipulation of label data within Python scripts. This format is particularly useful when you need to convert detections generated by custom models or dataloaders into a standardized structure before potentially converting them to other file-based formats supported by <code>labelformat</code>.</p>"},{"location":"formats/object-detection/labelformat/#specification-of-labelformat-detection-format","title":"Specification of Labelformat Detection Format","text":"<p>The format is defined by the <code>LabelformatObjectDetectionInput</code> dataclass in <code>labelformat.formats.labelformat</code>. It holds the following information directly in Python objects:</p> <ul> <li><code>categories</code>: A list of <code>Category</code> objects, each containing <code>id</code> and <code>name</code>.</li> <li><code>images</code>: A list of <code>Image</code> objects, each containing <code>id</code>, <code>filename</code>, <code>width</code>, and <code>height</code>.</li> <li><code>detections</code>: A list of <code>ImageObjectDetection</code> objects. Each object links an <code>Image</code> to a list of <code>SingleObjectDetection</code> instances.</li> <li><code>SingleObjectDetection</code> contains:<ul> <li><code>category</code>: The <code>Category</code> object for the detection.</li> <li><code>box</code>: A <code>BoundingBox</code> object representing the location (can be created from various formats like XYXY, XYWH, CXCYWH).</li> <li><code>confidence</code>: An optional float score (0-1).</li> </ul> </li> </ul> <p>Note: This format is primarily intended for programmatic use and does not support direct loading from files via the CLI using <code>add_cli_arguments</code>. It serves as a flexible in-memory structure.</p>"},{"location":"formats/object-detection/labelformat/#example-usage-programmatic-creation","title":"Example Usage (Programmatic Creation)","text":"<pre><code>from labelformat.formats.labelformat import LabelformatObjectDetectionInput\nfrom labelformat.model.bounding_box import BoundingBox, BoundingBoxFormat\nfrom labelformat.model.category import Category\nfrom labelformat.model.image import Image\nfrom labelformat.model.object_detection import (\n    ImageObjectDetection,\n    SingleObjectDetection,\n)\n\n# Assume you have:\n# my_dataloader: An iterable yielding (PILImage, filename)\n# my_model: A model with a .predict() method and .get_categories()\n# prediction_bbox_format: The format of your model's output boxes (\"xyxy\", \"xywh\", etc.)\n\ncategories = [\n    Category(id=i, name=category_name)\n    for i, category_name in enumerate(my_model.get_categories())\n]\ncategory_map = {cat.name: cat for cat in categories} # Or map by index if model outputs index\n\nimages = []\ndetections = []\nimage_id_counter = 0\n\nfor pil_image, filename in my_dataloader:\n    # Create Image object\n    current_image = Image(\n        id=image_id_counter,\n        filename=filename,\n        width=pil_image.width,\n        height=pil_image.height\n    )\n    images.append(current_image)\n\n    # Get model predictions\n    # Assuming predictions is a list of dicts like:\n    # [{'box': [x, y, w, h], 'category_id': 0, 'confidence': 0.9}]\n    model_predictions = my_model.predict(pil_image)\n\n    # Create SingleObjectDetection objects\n    objects = []\n    for pred in model_predictions:\n        objects.append(\n            SingleObjectDetection(\n                # Ensure you correctly map prediction category to Category object\n                category=categories[pred['category_id']],\n                box=BoundingBox.from_format(\n                    box=pred['box'],\n                    format=BoundingBoxFormat(prediction_bbox_format), # Use BoundingBoxFormat enum\n                ),\n                confidence=pred.get('confidence'), # Use .get for optional confidence\n            )\n        )\n\n    # Create ImageObjectDetection object\n    detections.append(\n        ImageObjectDetection(\n            image=current_image,\n            objects=objects,\n        )\n    )\n    image_id_counter += 1\n\n# Create the final input object\nlabelformat_input = LabelformatObjectDetectionInput(\n    categories=categories,\n    images=images,\n    labels=detections,\n)\n\n# Now labelformat_input can be used, e.g., to convert to another format:\n# from labelformat.formats.lightly import LightlyObjectDetectionOutput\n# output_converter = LightlyObjectDetectionOutput(output_folder=\"path/to/lightly_output\")\n# output_converter.save(labelformat_input)\n</code></pre>"},{"location":"formats/object-detection/lightly/","title":"Lightly Object Detection Format","text":""},{"location":"formats/object-detection/lightly/#overview","title":"Overview","text":"<p>The Lightly format is designed for efficient handling of object detection predictions in machine learning workflows. It provides a straightforward structure that's easy to parse and generate. For detailed information about the prediction format, refer to the Lightly AI documentation.</p>"},{"location":"formats/object-detection/lightly/#specification-of-lightly-detection-format","title":"Specification of Lightly Detection Format","text":"<p>The format uses a JSON file per image containing: - <code>file_name</code>: Name of the image file - <code>predictions</code>: List of object detections   - <code>category_id</code>: Integer ID of the object category   - <code>bbox</code>: List of [x, y, width, height] in absolute pixel coordinates   - <code>score</code>: Optional confidence score (0-1)</p>"},{"location":"formats/object-detection/lightly/#file-structure","title":"File Structure","text":"<pre><code>dataset/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2514\u2500\u2500 image2.jpg\n\u2514\u2500\u2500 predictions/\n    \u251c\u2500\u2500 image1.json\n    \u2514\u2500\u2500 image2.json\n</code></pre>"},{"location":"formats/object-detection/lightly/#example","title":"Example","text":"<pre><code>{\n  \"file_name\": \"image1.jpg\",\n  \"predictions\": [\n    {\n      \"category_id\": 0,\n      \"bbox\": [100, 200, 50, 30],\n      \"score\": 0.95\n    },\n    {\n      \"category_id\": 1,\n      \"bbox\": [300, 400, 80, 60],\n      \"score\": 0.87\n    }\n  ]\n}\n</code></pre>"},{"location":"formats/object-detection/pascalvoc/","title":"PascalVOC Object Detection Format","text":""},{"location":"formats/object-detection/pascalvoc/#overview","title":"Overview","text":"<p>PascalVOC (Visual Object Classes) is a widely used format for object detection tasks, introduced in the seminal paper \"The PASCAL Visual Object Classes (VOC) Challenge\" by Everingham et al. It stores annotations in XML files, with one XML file per image containing bounding box coordinates and class labels. The complete format specification is available in the PascalVOC development kit.</p>"},{"location":"formats/object-detection/pascalvoc/#specification","title":"Specification","text":"<p>Each XML annotation file contains: - Image metadata (filename, size, etc.) - List of objects, each with:   - Class name (string, allows spaces, e.g., \"traffic light\" or \"stop sign\")   - Bounding box coordinates as integer pixel values:     - xmin: left-most pixel coordinate     - ymin: top-most pixel coordinate     - xmax: right-most pixel coordinate     - ymax: bottom-most pixel coordinate   - Optional attributes (difficult, truncated, occluded)</p>"},{"location":"formats/object-detection/pascalvoc/#directory-structure","title":"Directory Structure","text":"<pre><code>dataset/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2514\u2500\u2500 image2.jpg\n\u2514\u2500\u2500 annotations/\n    \u251c\u2500\u2500 image1.xml\n    \u2514\u2500\u2500 image2.xml\n</code></pre>"},{"location":"formats/object-detection/pascalvoc/#example-annotation","title":"Example Annotation","text":"<pre><code>&lt;annotation&gt;\n    &lt;folder&gt;images&lt;/folder&gt;\n    &lt;filename&gt;image1.jpg&lt;/filename&gt;\n    &lt;size&gt;\n        &lt;width&gt;640&lt;/width&gt;\n        &lt;height&gt;480&lt;/height&gt;\n        &lt;depth&gt;3&lt;/depth&gt;\n    &lt;/size&gt;\n    &lt;object&gt;\n        &lt;name&gt;cat&lt;/name&gt;\n        &lt;pose&gt;Unspecified&lt;/pose&gt;\n        &lt;truncated&gt;0&lt;/truncated&gt;\n        &lt;difficult&gt;0&lt;/difficult&gt;\n        &lt;bndbox&gt;\n            &lt;xmin&gt;100&lt;/xmin&gt;\n            &lt;ymin&gt;200&lt;/ymin&gt;\n            &lt;xmax&gt;300&lt;/xmax&gt;\n            &lt;ymax&gt;400&lt;/ymax&gt;\n        &lt;/bndbox&gt;\n    &lt;/object&gt;\n&lt;/annotation&gt;\n</code></pre>"},{"location":"formats/object-detection/pascalvoc/#format-details","title":"Format Details","text":"<ul> <li>Coordinates are in absolute pixel values (not normalized)</li> <li>Bounding boxes use XYXY format (xmin, ymin, xmax, ymax)</li> <li>Each object can have optional attributes:</li> <li><code>difficult</code>: Indicates hard to recognize objects</li> <li><code>truncated</code>: Indicates objects partially outside the image</li> <li><code>occluded</code>: Indicates partially obscured objects</li> </ul>"},{"location":"formats/object-detection/pascalvoc/#converting-with-labelformat","title":"Converting with Labelformat","text":""},{"location":"formats/object-detection/pascalvoc/#coco-to-pascalvoc","title":"COCO to PascalVOC","text":"<pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file coco-labels/annotations.json \\\n    --output-format pascalvoc \\\n    --output-folder pascalvoc-labels\n</code></pre>"},{"location":"formats/object-detection/pascalvoc/#pascalvoc-to-coco","title":"PascalVOC to COCO","text":"<pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format pascalvoc \\\n    --input-folder pascalvoc-labels \\\n    --category-names cat,dog,fish \\\n    --output-format coco \\\n    --output-file coco-labels/annotations.json\n</code></pre>"},{"location":"formats/object-detection/pascalvoc/#required-arguments","title":"Required Arguments","text":"<ul> <li>For input:</li> <li><code>--input-folder</code>: Directory containing PascalVOC XML files</li> <li><code>--category-names</code>: Comma-separated list of category names (e.g., 'dog,cat')</li> <li>For output:</li> <li><code>--output-folder</code>: Directory to save generated XML files</li> </ul>"},{"location":"formats/object-detection/pascalvoc/#references","title":"References","text":"<ul> <li>Original PascalVOC Dataset</li> <li>Format Documentation </li> </ul>"},{"location":"formats/object-detection/rtdetr/","title":"RT-DETR Object Detection Format","text":""},{"location":"formats/object-detection/rtdetr/#overview","title":"Overview","text":"<p>RT-DETR (Real-Time DEtection TRansformer) is a groundbreaking end-to-end object detection framework introduced in the paper DETRs Beat YOLOs on Real-time Object Detection. RT-DETR represents the first real-time end-to-end object detector that successfully challenges the dominance of YOLO detectors in real-time applications. Unlike traditional detectors that require Non-Maximum Suppression (NMS) post-processing, RT-DETR eliminates NMS entirely while achieving superior speed and accuracy performance.</p> <p>Info: RT-DETR was introduced through the academic paper \"DETRs Beat YOLOs on Real-time Object Detection\" published in 2023.   For the full paper, see: arXiv:2304.08069   For implementation details and code, see: GitHub Repository: lyuwenyu/RT-DETR</p> <p>Availability: RT-DETR is now available in multiple frameworks:   - Hugging Face Transformers   - Ultralytics</p>"},{"location":"formats/object-detection/rtdetr/#key-rt-detr-model-features","title":"Key RT-DETR Model Features","text":"<p>RT-DETR uses the standard COCO annotation format while introducing revolutionary architectural innovations for real-time detection:</p> <ul> <li>End-to-End Architecture: First real-time detector to completely eliminate NMS post-processing, providing more stable and predictable inference times.</li> <li>Efficient Hybrid Encoder: Novel encoder design that decouples intra-scale interaction and cross-scale fusion to significantly reduce computational overhead.</li> <li>Uncertainty-Minimal Query Selection: Advanced query initialization scheme that optimizes both classification and localization confidence for improved detection quality.</li> <li>Flexible Speed Tuning: Supports adjustable inference speed by modifying the number of decoder layers without retraining.</li> <li>Superior Performance: Achieves state-of-the-art results (e.g., RT-DETR-R50 reaches 53.1% mAP @ 108 FPS on T4 GPU, outperforming YOLOv8-L in both speed and accuracy).</li> <li>Multiple Model Scales: Available in various scales (R18, R34, R50, R101) to accommodate different computational requirements.</li> </ul> <p>These architectural innovations are handled internally by the model design and training pipeline, requiring no changes to the standard COCO annotation format described below.</p>"},{"location":"formats/object-detection/rtdetr/#specification-of-rt-detr-detection-format","title":"Specification of RT-DETR Detection Format","text":"<p>RT-DETR uses the standard COCO format for annotations, ensuring seamless integration with existing COCO datasets and tools. The format consists of a single JSON file containing three main components:</p>"},{"location":"formats/object-detection/rtdetr/#images","title":"<code>images</code>","text":"<p>Defines metadata for each image in the dataset: <pre><code>{\n  \"id\": 0,                    // Unique image ID\n  \"file_name\": \"image1.jpg\",  // Image filename\n  \"width\": 640,               // Image width in pixels\n  \"height\": 416               // Image height in pixels\n}\n</code></pre></p>"},{"location":"formats/object-detection/rtdetr/#categories","title":"<code>categories</code>","text":"<p>Defines the object classes: <pre><code>{\n  \"id\": 0,                    // Unique category ID\n  \"name\": \"cat\"               // Category name\n}\n</code></pre></p>"},{"location":"formats/object-detection/rtdetr/#annotations","title":"<code>annotations</code>","text":"<p>Defines object instances: <pre><code>{\n  \"image_id\": 0,              // Reference to image\n  \"category_id\": 2,           // Reference to category\n  \"bbox\": [540.0, 295.0, 23.0, 18.0]  // [x, y, width, height] in absolute pixels\n}\n</code></pre></p>"},{"location":"formats/object-detection/rtdetr/#directory-structure-of-rt-detr-dataset","title":"Directory Structure of RT-DETR Dataset","text":"<pre><code>dataset/\n\u251c\u2500\u2500 images/                   # Image files\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2514\u2500\u2500 image2.jpg\n\u2514\u2500\u2500 annotations.json         # Single JSON file containing all annotations\n</code></pre>"},{"location":"formats/object-detection/rtdetr/#benefits-of-rt-detr-format","title":"Benefits of RT-DETR Format","text":"<ul> <li>Standard Compatibility: Uses the widely-adopted COCO format, ensuring compatibility with existing tools and frameworks.</li> <li>Flexibility: Supports adjustable inference speeds without retraining, making it adaptable to various real-time scenarios.</li> <li>Superior Accuracy: Achieves better accuracy than comparable YOLO detectors while maintaining competitive speed.</li> </ul>"},{"location":"formats/object-detection/rtdetr/#converting-annotations-to-rt-detr-format-with-labelformat","title":"Converting Annotations to RT-DETR Format with Labelformat","text":"<p>Since RT-DETR uses the standard COCO format, converting annotations to RT-DETR format is equivalent to converting to COCO format.</p>"},{"location":"formats/object-detection/rtdetr/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/rtdetr/#conversion-example-yolov8-to-rt-detr","title":"Conversion Example: YOLOv8 to RT-DETR","text":"<p>Assume you have annotations in YOLOv8 format and wish to convert them to RT-DETR. Here's how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard YOLOv8 structure with <code>data.yaml</code> and label files.</p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert YOLOv8 annotations to RT-DETR (COCO format): <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format yolov8 \\\n    --input-file dataset/data.yaml \\\n    --input-split train \\\n    --output-format rtdetr \\\n    --output-file dataset/rtdetr_annotations.json\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 rtdetr_annotations.json    # COCO format annotations for RT-DETR\n</code></pre></p>"},{"location":"formats/object-detection/rtdetr/#python-api-example","title":"Python API Example","text":"<pre><code>from pathlib import Path\nfrom labelformat.formats import YOLOv8ObjectDetectionInput, RTDETRObjectDetectionOutput\n\n# Load YOLOv8 format\nlabel_input = YOLOv8ObjectDetectionInput(\n    input_file=Path(\"dataset/data.yaml\"),\n    input_split=\"train\"\n)\n\n# Convert to RT-DETR format\nRTDETRObjectDetectionOutput(\n    output_file=Path(\"dataset/rtdetr_annotations.json\")\n).save(label_input=label_input)\n</code></pre>"},{"location":"formats/object-detection/rtdetr/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>Since RT-DETR uses the COCO format, the same validation and error handling applies:</p> <ul> <li>Invalid JSON Structure: Proper error reporting for malformed JSON files</li> <li>Missing Required Fields: Validation ensures all required COCO fields are present</li> <li>Reference Integrity: Checks that image_id and category_id references are valid</li> <li>Bounding Box Validation: Ensures bounding boxes are within image boundaries</li> </ul> <p>Example of a properly formatted annotation: <pre><code>{\n  \"images\": [{\"id\": 0, \"file_name\": \"image1.jpg\", \"width\": 640, \"height\": 480}],\n  \"categories\": [{\"id\": 1, \"name\": \"person\"}],\n  \"annotations\": [{\"image_id\": 0, \"category_id\": 1, \"bbox\": [100, 120, 50, 80]}]\n}\n</code></pre></p>"},{"location":"formats/object-detection/rtdetrv2/","title":"RT-DETRv2 Object Detection Format","text":""},{"location":"formats/object-detection/rtdetrv2/#overview","title":"Overview","text":"<p>RT-DETRv2 is an enhanced version of the Real-Time DEtection TRansformer (RT-DETR), introduced in the paper RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer. Building upon the groundbreaking end-to-end object detection framework of the original RT-DETR, RT-DETRv2 continues the legacy of eliminating Non-Maximum Suppression (NMS) post-processing while introducing additional improvements in accuracy and efficiency for real-time object detection scenarios.</p> <p>Info: RT-DETRv2 was introduced through the technical report \"RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer\" published in 2024.   For the full paper, see: arXiv:2407.17140   For RT-DETR foundation, see: RT-DETR Paper (arXiv:2304.08069)   For implementation details and code, see: GitHub Repository: lyuwenyu/RT-DETR</p> <p>Availability: RT-DETRv2 is now available in multiple frameworks:   - Hugging Face Transformers   - Ultralytics</p>"},{"location":"formats/object-detection/rtdetrv2/#key-rt-detrv2-model-features","title":"Key RT-DETRv2 Model Features","text":"<p>RT-DETRv2 maintains compatibility with the standard COCO annotation format while introducing specific technical improvements over RT-DETR:</p> <ul> <li>Distinct Sampling Points for Different Scales: Introduces flexible multi-scale feature extraction by setting different numbers of sampling points for features at different scales in the deformable attention module, rather than using the same number across all scales.</li> <li>Discrete Sampling Operator: Provides an optional discrete sampling operator to replace the grid_sample operator, removing deployment constraints typically associated with DETRs and improving practical applicability across different deployment platforms.</li> <li>Dynamic Data Augmentation: Implements adaptive data augmentation strategy that applies stronger augmentation in early training periods and reduces it in later stages to improve model robustness and target domain adaptation.</li> <li>Scale-Adaptive Hyperparameters: Customizes optimizer hyperparameters based on model scale, using higher learning rates for lighter models (e.g., ResNet18) and lower rates for larger models (e.g., ResNet101) to achieve optimal performance.</li> <li>Bag-of-Freebies Approach: Incorporates multiple training improvements that enhance performance without increasing inference cost or model complexity.</li> <li>Consistent Performance Gains: Achieves improved accuracy across all model scales (S: +1.4 mAP, M: +1.0 mAP, L: +0.3 mAP) while maintaining the same inference speed as RT-DETR.</li> </ul> <p>These enhancements are handled internally by the model design and training pipeline, requiring no changes to the standard COCO annotation format described below.</p>"},{"location":"formats/object-detection/rtdetrv2/#specification-of-rt-detrv2-detection-format","title":"Specification of RT-DETRv2 Detection Format","text":"<p>RT-DETRv2 uses the standard COCO format for annotations, ensuring complete compatibility with existing COCO datasets and tools. The format specification is identical to the original COCO format:</p>"},{"location":"formats/object-detection/rtdetrv2/#images","title":"<code>images</code>","text":"<p>Defines metadata for each image in the dataset: <pre><code>{\n  \"id\": 0,                    // Unique image ID\n  \"file_name\": \"image1.jpg\",  // Image filename\n  \"width\": 640,               // Image width in pixels\n  \"height\": 416               // Image height in pixels\n}\n</code></pre></p>"},{"location":"formats/object-detection/rtdetrv2/#categories","title":"<code>categories</code>","text":"<p>Defines the object classes: <pre><code>{\n  \"id\": 0,                    // Unique category ID\n  \"name\": \"cat\"               // Category name\n}\n</code></pre></p>"},{"location":"formats/object-detection/rtdetrv2/#annotations","title":"Annotations","text":"<p>Defines object instances: <pre><code>{\n  \"image_id\": 0,              // Reference to image\n  \"category_id\": 2,           // Reference to category\n  \"bbox\": [540.0, 295.0, 23.0, 18.0]  // [x, y, width, height] in absolute pixels\n}\n</code></pre></p>"},{"location":"formats/object-detection/rtdetrv2/#directory-structure-of-rt-detrv2-dataset","title":"Directory Structure of RT-DETRv2 Dataset","text":"<pre><code>dataset/\n\u251c\u2500\u2500 images/                   # Image files\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2514\u2500\u2500 image2.jpg\n\u2514\u2500\u2500 annotations.json         # Single JSON file containing all annotations\n</code></pre>"},{"location":"formats/object-detection/rtdetrv2/#benefits-of-rt-detrv2-format","title":"Benefits of RT-DETRv2 Format","text":"<ul> <li>Standard Compatibility: Uses the widely-adopted COCO format, ensuring compatibility with existing tools and frameworks.</li> <li>End-to-End Processing: Maintains the NMS-free architecture for stable and predictable inference performance.</li> <li>Enhanced Performance: Improved accuracy and efficiency compared to the original RT-DETR.</li> </ul>"},{"location":"formats/object-detection/rtdetrv2/#converting-annotations-to-rt-detrv2-format-with-labelformat","title":"Converting Annotations to RT-DETRv2 Format with Labelformat","text":"<p>Since RT-DETRv2 uses the standard COCO format, converting annotations to RT-DETRv2 format is equivalent to converting to COCO format.</p>"},{"location":"formats/object-detection/rtdetrv2/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/rtdetrv2/#conversion-example-yolov8-to-rt-detrv2","title":"Conversion Example: YOLOv8 to RT-DETRv2","text":"<p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard YOLOv8 structure with <code>data.yaml</code> and label files.</p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert YOLOv8 annotations to RT-DETRv2 (COCO format): <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format yolov8 \\\n    --input-file dataset/data.yaml \\\n    --input-split train \\\n    --output-format rtdetrv2 \\\n    --output-file dataset/rtdetrv2_annotations.json\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 rtdetrv2_annotations.json    # COCO format annotations for RT-DETRv2\n</code></pre></p>"},{"location":"formats/object-detection/rtdetrv2/#python-api-example","title":"Python API Example","text":"<pre><code>from pathlib import Path\nfrom labelformat.formats import YOLOv8ObjectDetectionInput, RTDETRv2ObjectDetectionOutput\n\n# Load YOLOv8 format\nlabel_input = YOLOv8ObjectDetectionInput(\n    input_file=Path(\"dataset/data.yaml\"),\n    input_split=\"train\"\n)\n\n# Convert to RT-DETRv2 format\nRTDETRv2ObjectDetectionOutput(\n    output_file=Path(\"dataset/rtdetrv2_annotations.json\")\n).save(label_input=label_input)\n</code></pre>"},{"location":"formats/object-detection/rtdetrv2/#rt-detrv2-vs-rt-detr","title":"RT-DETRv2 vs RT-DETR","text":"<p>RT-DETRv2 builds upon the foundation of RT-DETR with several key improvements:</p> <ul> <li>Enhanced Architecture: Refined encoder and decoder designs for better performance</li> <li>Improved Training: Advanced training strategies and optimization techniques</li> <li>Better Accuracy: Higher detection accuracy across various model scales</li> </ul>"},{"location":"formats/object-detection/rtdetrv2/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>Since RT-DETRv2 uses the COCO format, the same validation and error handling applies:</p> <ul> <li>Invalid JSON Structure: Proper error reporting for malformed JSON files</li> <li>Missing Required Fields: Validation ensures all required COCO fields are present</li> <li>Invalid JSON Structure: Proper error reporting for malformed JSON files.</li> <li>Missing Required Fields: Validation ensures all required COCO fields are present.</li> <li>Reference Integrity: Checks that image_id and category_id references are valid.</li> <li>Bounding Box Validation: Ensures bounding boxes are within image boundaries. <pre><code>{\n  \"images\": [{\"id\": 0, \"file_name\": \"image1.jpg\", \"width\": 640, \"height\": 480}],\n  \"categories\": [{\"id\": 1, \"name\": \"person\"}],\n  \"annotations\": [{\"image_id\": 0, \"category_id\": 1, \"bbox\": [100, 120, 50, 80]}]\n}\n</code></pre></li> </ul>"},{"location":"formats/object-detection/yolov10/","title":"YOLOv10 Object Detection Format","text":""},{"location":"formats/object-detection/yolov10/#overview","title":"Overview","text":"<p>YOLOv10 is a well-known object detection model in the You Only Look Once (YOLO) series, introduced in the paper YOLOv10: Real-Time End-to-End Object Detection. Building upon the foundations of YOLOv5 through YOLOv9, YOLOv10 introduces significant advancements in model architecture and training methodologies, enhancing both accuracy and efficiency. Despite these improvements, YOLOv10 retains the same object detection format as its predecessors, utilizing normalized coordinates in text files. This consistency ensures seamless integration and compatibility with existing workflows while benefiting from YOLOv10's enhanced performance.</p>"},{"location":"formats/object-detection/yolov10/#key-yolov10-model-features","title":"Key YOLOv10 Model Features","text":"<p>YOLOv10 maintains full compatibility with the label format used in YOLOv5-v9, while introducing several key innovations:</p> <ul> <li>Adaptive Feature Fusion: Novel architecture that dynamically adjusts feature importance based on input complexity, leading to improved detection accuracy across varying scales.</li> <li>Resource-Aware Inference: Intelligent compute allocation that reduces GPU memory usage by up to 30% without sacrificing detection performance.</li> <li>Enhanced Small Object Detection: Specialized attention mechanisms that boost detection accuracy for small objects while maintaining real-time inference speeds.</li> <li>Automated Architecture Search: Built-in neural architecture search capabilities that optimize model configurations for specific hardware platforms.</li> </ul> <p>These improvements are handled internally by the model architecture and training pipeline, requiring no changes to the basic annotation format described below.</p>"},{"location":"formats/object-detection/yolov10/#specification-of-yolov10-detection-format","title":"Specification of YOLOv10 Detection Format","text":"<p>The YOLOv10 detection format remains consistent with previous versions (v5-v9), ensuring ease of adoption and compatibility. Below are the detailed specifications:</p> <ul> <li> <p>One Text File per Image:   For every image in your dataset, there exists a corresponding <code>.txt</code> file containing annotation data.</p> </li> <li> <p>Object Representation:   Each line in the text file represents a single object detected within the image, following the format: <code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> </p> <ul> <li><code>&lt;class_id&gt;</code> (Integer):   An integer representing the object's class.</li> <li><code>&lt;x_center&gt;</code> and <code>&lt;y_center&gt;</code> (Float): The normalized coordinates of the object's center relative to the image's width and    height.</li> <li><code>&lt;width&gt;</code> and <code>&lt;height&gt;</code> (Float): The normalized width and height of the bounding box encompassing the object.</li> </ul> </li> <li> <p>Normalization of Values:   All coordinate and size values are normalized to a range between <code>0.0</code> and <code>1.0</code>.  </p> </li> <li> <p>Normalization Formula:     To convert pixel values to normalized coordinates: <pre><code>normalized_x = x_pixel / image_width  \nnormalized_y = y_pixel / image_height  \nnormalized_width = box_width_pixel / image_width  \nnormalized_height = box_height_pixel / image_height\n</code></pre></p> </li> <li> <p>Class ID Indexing:   Class IDs start from <code>0</code>, with each ID corresponding to a specific object category defined in the <code>data.yaml</code> configuration file. Class IDs must be contiguous integers (e.g., 0,1,2 and not 0,2,3) to ensure proper model training and inference.</p> </li> <li> <p>Configuration via <code>data.yaml</code>:   The <code>data.yaml</code> file contains essential configuration settings, including paths to training and validation datasets, number of classes (<code>nc</code>), and a mapping of class names to their respective IDs (<code>names</code>).</p> </li> </ul>"},{"location":"formats/object-detection/yolov10/#directory-structure-of-yolov10-dataset","title":"Directory Structure of YOLOv10 Dataset","text":"<p>The dataset must maintain a parallel directory structure for images and their corresponding label files. There are two common organizational patterns:</p>"},{"location":"formats/object-detection/yolov10/#pattern-1-images-and-labels-as-root-directories","title":"Pattern 1: Images and Labels as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 image3.jpg\n\u2502       \u2514\u2500\u2500 image4.jpg\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 image1.txt\n    \u2502   \u2514\u2500\u2500 image2.txt\n    \u2514\u2500\u2500 val/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre>"},{"location":"formats/object-detection/yolov10/#pattern-2-trainval-as-root-directories","title":"Pattern 2: Train/Val as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image3.jpg\n    \u2502   \u2514\u2500\u2500 image4.jpg\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre> <p>The corresponding <code>data.yaml</code> configuration should match your chosen structure:</p> <pre><code># For Pattern 1:\npath: .  # Optional - defaults to current directory if omitted\ntrain: images/train  # Path to training images\nval: images/val      # Path to validation images\n\n# For Pattern 2:\npath: .  # Optional - defaults to current directory if omitted\ntrain: train/images  # Path to training images\nval: val/images      # Path to validation images\n</code></pre> <p>Important: Label files must have the same name as their corresponding image files (excluding the file extension) and must maintain the parallel directory structure, only replacing <code>images</code> with <code>labels</code> in the path. </p>"},{"location":"formats/object-detection/yolov10/#benefits-of-yolov10-format","title":"Benefits of YOLOv10 Format","text":"<ul> <li>Simplicity: Easy to read and write, facilitating quick dataset preparation.</li> <li>Efficiency: Compact representation reduces storage requirements.</li> <li>Compatibility: Maintains consistency across YOLO versions, ensuring seamless integration with various tools and frameworks.</li> </ul>"},{"location":"formats/object-detection/yolov10/#example-of-yolov10-format","title":"Example of YOLOv10 Format","text":"<p>Example of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory (defaults to current directory if omitted). Can also be `../dataset`.\ntrain: images/train  # Directory for training images\nval: images/val  # Directory for validation images\ntest: images/test  # Directory for test images (optional)\n\nnames:\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Example Annotation</p> <p>For an image named <code>image1.jpg</code>, the corresponding <code>image1.txt</code> might contain: <pre><code>0 0.716797 0.395833 0.216406 0.147222\n1 0.687500 0.379167 0.255208 0.175000\n</code></pre></p> <p>Explanation: - The first line represents an object of class <code>0</code> (e.g., <code>cat</code>) with its bounding box centered at <code>(0.716797, 0.395833)</code> relative to the image dimensions, and a width and height of <code>0.216406</code> and <code>0.147222</code> respectively. - The second line represents an object of class <code>1</code> (e.g., <code>dog</code>) with its own bounding box specifications.</p>"},{"location":"formats/object-detection/yolov10/#normalizing-bounding-box-coordinates-for-yolov10","title":"Normalizing Bounding Box Coordinates for YOLOv10","text":"<p>To convert pixel values to normalized values required by YOLOv10:</p> <pre><code># Given pixel values and image dimensions\nx_top_left = 150     # x coordinate of top-left corner of bounding box\ny_top_left = 200     # y coordinate of top-left corner of bounding box\nwidth_pixel = 50     # width of bounding box\nheight_pixel = 80    # height of bounding box\nimage_width = 640\nimage_height = 480\n\n# 1. Convert top-left coordinates to center coordinates (in pixels)\nx_center_pixel = x_top_left + (width_pixel / 2)   # 150 + (50/2) = 175\ny_center_pixel = y_top_left + (height_pixel / 2)  # 200 + (80/2) = 240\n\n# 2. Normalize all values (divide by image dimensions)\nx_center = x_center_pixel / image_width     # 175 / 640 = 0.273438\ny_center = y_center_pixel / image_height    # 240 / 480 = 0.500000\nwidth = width_pixel / image_width           # 50 / 640 = 0.078125\nheight = height_pixel / image_height        # 80 / 480 = 0.166667\n\n# Annotation line format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\nannotation = f\"0 {x_center} {y_center} {width} {height}\"\n# Output: \"0 0.273438 0.500000 0.078125 0.166667\"\n</code></pre>"},{"location":"formats/object-detection/yolov10/#converting-annotations-to-yolov10-format-with-labelformat","title":"Converting Annotations to YOLOv10 Format with Labelformat","text":"<p>Our Labelformat framework simplifies the process of converting various annotation formats to the YOLOv10 detection format. Below is a step-by-step guide to perform this conversion.</p>"},{"location":"formats/object-detection/yolov10/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed. You can install it via pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/yolov10/#conversion-example-coco-to-yolov10","title":"Conversion Example: COCO to YOLOv10","text":"<p>Assume you have annotations in the COCO format and wish to convert them to YOLOv10. Here's how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard COCO structure:</p> <ul> <li>You have a <code>.json</code> file with the COCO annotations. (e.g. <code>annotations/instances_train.json</code>)</li> <li>You have a directory with the images. (e.g. <code>images/</code>)</li> </ul> <p>Full example: <pre><code>dataset/\n\u251c\u2500\u2500 annotations/\n\u2502   \u2514\u2500\u2500 instances_train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n</code></pre></p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert COCO annotations to YOLOv10: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov10 \\\n    --output-folder dataset/yolov10_labels \\\n    --output-split train\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 yolov10_labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n</code></pre></p> <p>Contents of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory\ntrain: images  # Directory for training images\nnc: 3  # Number of classes\nnames:  # Class name mapping\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Contents of <code>image1.txt</code>: <pre><code>0 0.234375 0.416667 0.078125 0.166667\n1 0.500000 0.500000 0.100000 0.200000\n</code></pre></p>"},{"location":"formats/object-detection/yolov10/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>The format implementation includes several safeguards:</p> <ul> <li>Missing Label Files:</li> <li>Warning is logged if a label file doesn't exist for an image</li> <li> <p>Image is skipped from processing</p> </li> <li> <p>File Access Issues:</p> </li> <li>Errors are logged if label files cannot be read due to permissions or other OS issues</li> <li> <p>Affected images are skipped from processing</p> </li> <li> <p>Label Format Validation:</p> </li> <li>Each line must contain exactly 5 space-separated values</li> <li>Invalid lines are logged as warnings and skipped</li> <li>All values must be convertible to appropriate types:<ul> <li>Category ID must be a valid integer</li> <li>Coordinates and dimensions must be valid floats</li> </ul> </li> <li>Category IDs must exist in the category mapping</li> </ul> <p>Example of a properly formatted label file: <pre><code>0 0.716797 0.395833 0.216406 0.147222  # Each line must have exactly 5 space-separated values\n1 0.687500 0.379167 0.255208 0.175000  # All values must be valid numbers within [0,1] range\n</code></pre></p>"},{"location":"formats/object-detection/yolov11/","title":"YOLOv11 Object Detection Format","text":""},{"location":"formats/object-detection/yolov11/#overview","title":"Overview","text":"<p>YOLOv11 is the latest iteration in the You Only Look Once (YOLO) series, renowned for its real-time object detection capabilities. Building upon the foundations of YOLOv5 through YOLOv10, YOLOv11 introduces significant advancements in model architecture and training methodologies, enhancing both accuracy and efficiency. Despite these improvements, YOLOv11 retains the same object detection format as its predecessors, utilizing normalized coordinates in text files. This consistency ensures seamless integration and compatibility with existing workflows while benefiting from YOLOv11's enhanced performance.</p> <p>Info: Unlike other YOLO versions, YOLOv11 was not introduced through an academic paper. The most commonly referenced implementation is the Ultralytics YOLOv11 repository on GitHub, which has become the de facto standard implementation.    For implementation details and specifications, see: GitHub Repository: ultralytics/yolov11</p>"},{"location":"formats/object-detection/yolov11/#key-yolov11-model-features","title":"Key YOLOv11 Model Features","text":"<p>YOLOv11 maintains full compatibility with the label format used in YOLOv5-v10, while introducing several groundbreaking capabilities:</p> <ul> <li>Enhanced Instance Segmentation: YOLOv11 adds support for polygon-based instance segmentation annotations (documented separately in the instance-segmentation format guide).</li> <li>Multi-Task Learning: The model can simultaneously handle object detection, instance segmentation, and pose estimation tasks using the same base format with task-specific extensions.</li> <li>Advanced Data Augmentation: While the base format remains the same, YOLOv11 introduces new augmentation techniques that can be applied during training without modifying the original annotations.</li> </ul> <p>These advanced features are handled internally by the model architecture and training pipeline, requiring no changes to the basic annotation format described below.</p>"},{"location":"formats/object-detection/yolov11/#specification-of-yolov11-detection-format","title":"Specification of YOLOv11 Detection Format","text":"<p>The YOLOv11 detection format remains consistent with previous versions (v5-v10), ensuring ease of adoption and compatibility. Below are the detailed specifications:</p> <ul> <li> <p>One Text File per Image:   For every image in your dataset, there exists a corresponding <code>.txt</code> file containing annotation data.</p> </li> <li> <p>Object Representation:   Each line in the text file represents a single object detected within the image, following the format: <code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> </p> <ul> <li><code>&lt;class_id&gt;</code> (Integer):   An integer representing the object's class.</li> <li><code>&lt;x_center&gt;</code> and <code>&lt;y_center&gt;</code> (Float): The normalized coordinates of the object's center relative to the image's width and    height.</li> <li><code>&lt;width&gt;</code> and <code>&lt;height&gt;</code> (Float): The normalized width and height of the bounding box encompassing the object.</li> </ul> </li> <li> <p>Normalization of Values:   All coordinate and size values are normalized to a range between <code>0.0</code> and <code>1.0</code>.  </p> </li> <li> <p>Normalization Formula:     To convert pixel values to normalized coordinates: <pre><code>normalized_x = x_pixel / image_width  \nnormalized_y = y_pixel / image_height  \nnormalized_width = box_width_pixel / image_width  \nnormalized_height = box_height_pixel / image_height\n</code></pre></p> </li> <li> <p>Class ID Indexing:   Class IDs start from <code>0</code>, with each ID corresponding to a specific object category defined in the <code>data.yaml</code> configuration file. Class IDs must be contiguous integers (e.g., 0,1,2 and not 0,2,3) to ensure proper model training and inference.</p> </li> <li> <p>Configuration via <code>data.yaml</code>:   The <code>data.yaml</code> file contains essential configuration settings, including paths to training and validation datasets, number of classes (<code>nc</code>), and a mapping of class names to their respective IDs (<code>names</code>).</p> </li> </ul>"},{"location":"formats/object-detection/yolov11/#directory-structure-of-yolov11-dataset","title":"Directory Structure of YOLOv11 Dataset","text":"<p>The dataset must maintain a parallel directory structure for images and their corresponding label files. There are two common organizational patterns:</p>"},{"location":"formats/object-detection/yolov11/#pattern-1-images-and-labels-as-root-directories","title":"Pattern 1: Images and Labels as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 image3.jpg\n\u2502       \u2514\u2500\u2500 image4.jpg\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 image1.txt\n    \u2502   \u2514\u2500\u2500 image2.txt\n    \u2514\u2500\u2500 val/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre>"},{"location":"formats/object-detection/yolov11/#pattern-2-trainval-as-root-directories","title":"Pattern 2: Train/Val as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image3.jpg\n    \u2502   \u2514\u2500\u2500 image4.jpg\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre> <p>The corresponding <code>data.yaml</code> configuration should match your chosen structure:</p> <pre><code># For Pattern 1:\npath: .  # Optional - defaults to current directory if omitted\ntrain: images/train  # Path to training images\nval: images/val      # Path to validation images\n\n# For Pattern 2:\npath: .  # Optional - defaults to current directory if omitted\ntrain: train/images  # Path to training images\nval: val/images      # Path to validation images\n</code></pre> <p>Important: Label files must have the same name as their corresponding image files (excluding the file extension) and must maintain the parallel directory structure, only replacing <code>images</code> with <code>labels</code> in the path. </p>"},{"location":"formats/object-detection/yolov11/#benefits-of-yolov11-format","title":"Benefits of YOLOv11 Format","text":"<ul> <li>Simplicity: Easy to read and write, facilitating quick dataset preparation.</li> <li>Efficiency: Compact representation reduces storage requirements.</li> <li>Compatibility: Maintains consistency across YOLO versions, ensuring seamless integration with various tools and frameworks.</li> </ul>"},{"location":"formats/object-detection/yolov11/#example-of-yolov11-format","title":"Example of YOLOv11 Format","text":"<p>Example of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory (defaults to current directory if omitted). Can also be `../dataset`.\ntrain: images/train  # Directory for training images\nval: images/val  # Directory for validation images\ntest: images/test  # Directory for test images (optional)\n\nnames:\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Example Annotation</p> <p>For an image named <code>image1.jpg</code>, the corresponding <code>image1.txt</code> might contain: <pre><code>0 0.716797 0.395833 0.216406 0.147222\n1 0.687500 0.379167 0.255208 0.175000\n</code></pre></p> <p>Explanation: - The first line represents an object of class <code>0</code> (e.g., <code>cat</code>) with its bounding box centered at <code>(0.716797, 0.395833)</code> relative to the image dimensions, and a width and height of <code>0.216406</code> and <code>0.147222</code> respectively. - The second line represents an object of class <code>1</code> (e.g., <code>dog</code>) with its own bounding box specifications.</p>"},{"location":"formats/object-detection/yolov11/#normalizing-bounding-box-coordinates-for-yolov11","title":"Normalizing Bounding Box Coordinates for YOLOv11","text":"<p>To convert pixel values to normalized values required by YOLOv11:</p> <pre><code># Given pixel values and image dimensions\nx_top_left = 150     # x coordinate of top-left corner of bounding box\ny_top_left = 200     # y coordinate of top-left corner of bounding box\nwidth_pixel = 50     # width of bounding box\nheight_pixel = 80    # height of bounding box\nimage_width = 640\nimage_height = 480\n\n# 1. Convert top-left coordinates to center coordinates (in pixels)\nx_center_pixel = x_top_left + (width_pixel / 2)   # 150 + (50/2) = 175\ny_center_pixel = y_top_left + (height_pixel / 2)  # 200 + (80/2) = 240\n\n# 2. Normalize all values (divide by image dimensions)\nx_center = x_center_pixel / image_width     # 175 / 640 = 0.273438\ny_center = y_center_pixel / image_height    # 240 / 480 = 0.500000\nwidth = width_pixel / image_width           # 50 / 640 = 0.078125\nheight = height_pixel / image_height        # 80 / 480 = 0.166667\n\n# Annotation line format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\nannotation = f\"0 {x_center} {y_center} {width} {height}\"\n# Output: \"0 0.273438 0.500000 0.078125 0.166667\"\n</code></pre>"},{"location":"formats/object-detection/yolov11/#converting-annotations-to-yolov11-format-with-labelformat","title":"Converting Annotations to YOLOv11 Format with Labelformat","text":"<p>Our Labelformat framework simplifies the process of converting various annotation formats to the YOLOv11 detection format. Below is a step-by-step guide to perform this conversion.</p>"},{"location":"formats/object-detection/yolov11/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed. You can install it via pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/yolov11/#conversion-example-coco-to-yolov11","title":"Conversion Example: COCO to YOLOv11","text":"<p>Assume you have annotations in the COCO format and wish to convert them to YOLOv11. Here's how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard COCO structure:</p> <ul> <li>You have a <code>.json</code> file with the COCO annotations. (e.g. <code>annotations/instances_train.json</code>)</li> <li>You have a directory with the images. (e.g. <code>images/</code>)</li> </ul> <p>Full example: <pre><code>dataset/\n\u251c\u2500\u2500 annotations/\n\u2502   \u2514\u2500\u2500 instances_train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n</code></pre></p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert COCO annotations to YOLOv11: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov11 \\\n    --output-folder dataset/yolov11_labels \\\n    --output-split train\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 yolov11_labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n</code></pre></p> <p>Contents of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory\ntrain: images  # Directory for training images\nnc: 3  # Number of classes\nnames:  # Class name mapping\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Contents of <code>image1.txt</code>: <pre><code>0 0.234375 0.416667 0.078125 0.166667\n1 0.500000 0.500000 0.100000 0.200000\n</code></pre></p>"},{"location":"formats/object-detection/yolov11/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>The format implementation includes several safeguards:</p> <ul> <li>Missing Label Files:</li> <li>Warning is logged if a label file doesn't exist for an image</li> <li> <p>Image is skipped from processing</p> </li> <li> <p>File Access Issues:</p> </li> <li>Errors are logged if label files cannot be read due to permissions or other OS issues</li> <li> <p>Affected images are skipped from processing</p> </li> <li> <p>Label Format Validation:</p> </li> <li>Each line must contain exactly 5 space-separated values</li> <li>Invalid lines are logged as warnings and skipped</li> <li>All values must be convertible to appropriate types:<ul> <li>Category ID must be a valid integer</li> <li>Coordinates and dimensions must be valid floats</li> </ul> </li> <li>Category IDs must exist in the category mapping</li> </ul> <p>Example of a properly formatted label file: <pre><code>0 0.716797 0.395833 0.216406 0.147222  # Each line must have exactly 5 space-separated values\n1 0.687500 0.379167 0.255208 0.175000  # All values must be valid numbers within [0,1] range\n</code></pre></p>"},{"location":"formats/object-detection/yolov12/","title":"YOLOv12 Object Detection Format","text":""},{"location":"formats/object-detection/yolov12/#overview","title":"Overview","text":"<p>YOLOv12 is a groundbreaking iteration in the You Only Look Once (YOLO) series, introduced in the paper YOLOv12: Attention-Centric Real-Time Object Detectors. YOLOv12 represents the first attention-centric YOLO framework, departing from the traditional CNN-based approach while maintaining the real-time performance characteristics that define the YOLO series. Building upon the foundations of YOLOv5 through YOLOv11, YOLOv12 introduces revolutionary attention mechanisms and architectural innovations that achieve state-of-the-art accuracy with competitive inference speeds. Despite these architectural advances, YOLOv12 retains the same object detection format as its predecessors, utilizing normalized coordinates in text files for seamless integration with existing workflows.</p> <p>Info: YOLOv12 was introduced through the academic paper \"YOLOv12: Attention-Centric Real-Time Object Detectors\" published in 2025. The official implementation is available on GitHub.   For the full paper, see: arXiv:2502.12524   For implementation details and code, see: GitHub Repository: sunsmarterjie/yolov12</p>"},{"location":"formats/object-detection/yolov12/#key-yolov12-model-features","title":"Key YOLOv12 Model Features","text":"<p>YOLOv12 maintains full compatibility with the label format used in YOLOv5-v11, while introducing revolutionary architectural innovations:</p> <ul> <li>Attention-Centric Architecture: The first YOLO model to prioritize attention mechanisms over traditional CNNs, leveraging the superior modeling capabilities of attention while maintaining real-time performance.</li> <li>Area Attention (A\u00b2) Module: A novel attention mechanism that reduces computational complexity from O(n\u00b2) to efficient levels while maintaining large receptive fields through simple area-based partitioning.</li> <li>R-ELAN (Residual Efficient Layer Aggregation Networks): Enhanced feature aggregation with residual connections and scaling techniques, addressing optimization challenges in attention-based architectures.</li> <li>FlashAttention Integration: Optimized memory access patterns that solve the memory bottleneck issues of attention mechanisms, enabling efficient inference on modern GPUs.</li> <li>Superior Performance: Achieves significant improvements over previous YOLO versions (e.g., YOLOv12-N reaches 40.6% mAP, outperforming YOLOv11-N by 1.2% mAP with comparable speed).</li> <li>Multi-Scale Excellence: Consistent performance gains across all model scales (N, S, M, L, X) while maintaining or reducing computational requirements.</li> </ul> <p>These architectural innovations are handled internally by the model design and training pipeline, requiring no changes to the basic annotation format described below.</p>"},{"location":"formats/object-detection/yolov12/#hardware-requirements","title":"Hardware Requirements","text":"<p>Important: YOLOv12 requires FlashAttention for optimal performance, which currently supports specific GPU architectures:</p> <ul> <li>Supported GPUs: Turing, Ampere, Ada Lovelace, or Hopper architectures</li> <li>Examples: T4, Quadro RTX series, RTX 20/30/40 series, RTX A5000/A6000, A30/A40, A100, H100</li> <li>Note: Older GPU architectures may not achieve the full performance benefits of YOLOv12's attention mechanisms</li> </ul> <p>For deployment on unsupported hardware, consider using YOLOv11 or earlier versions for optimal performance.</p>"},{"location":"formats/object-detection/yolov12/#specification-of-yolov12-detection-format","title":"Specification of YOLOv12 Detection Format","text":"<p>The YOLOv12 detection format remains consistent with previous versions (v5-v11), ensuring ease of adoption and compatibility. Below are the detailed specifications:</p> <ul> <li> <p>One Text File per Image:   For every image in your dataset, there exists a corresponding <code>.txt</code> file containing annotation data.</p> </li> <li> <p>Object Representation:   Each line in the text file represents a single object detected within the image, following the format: <code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code></p> <ul> <li><code>&lt;class_id&gt;</code> (Integer): An integer representing the object's class.</li> <li><code>&lt;x_center&gt;</code> and <code>&lt;y_center&gt;</code> (Float): The normalized coordinates of the object's center relative to the image's width and height.</li> <li><code>&lt;width&gt;</code> and <code>&lt;height&gt;</code> (Float): The normalized width and height of the bounding box encompassing the object.</li> </ul> </li> <li> <p>Normalization of Values:   All coordinate and size values are normalized to a range between <code>0.0</code> and <code>1.0</code>.</p> </li> <li> <p>Normalization Formula:     To convert pixel values to normalized coordinates:     <pre><code>normalized_x = x_pixel / image_width\nnormalized_y = y_pixel / image_height\nnormalized_width = box_width_pixel / image_width\nnormalized_height = box_height_pixel / image_height\n</code></pre></p> </li> <li> <p>Class ID Indexing:   Class IDs start from <code>0</code>, with each ID corresponding to a specific object category defined in the <code>data.yaml</code> configuration file. Class IDs must be contiguous integers (e.g., 0,1,2 and not 0,2,3) to ensure proper model training and inference.</p> </li> <li> <p>Configuration via <code>data.yaml</code>:   The <code>data.yaml</code> file contains essential configuration settings, including paths to training and validation datasets, number of classes (<code>nc</code>), and a mapping of class names to their respective IDs (<code>names</code>).</p> </li> </ul>"},{"location":"formats/object-detection/yolov12/#directory-structure-of-yolov12-dataset","title":"Directory Structure of YOLOv12 Dataset","text":"<p>The dataset must maintain a parallel directory structure for images and their corresponding label files. There are two common organizational patterns:</p>"},{"location":"formats/object-detection/yolov12/#pattern-1-images-and-labels-as-root-directories","title":"Pattern 1: Images and Labels as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 image3.jpg\n\u2502       \u2514\u2500\u2500 image4.jpg\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 image1.txt\n    \u2502   \u2514\u2500\u2500 image2.txt\n    \u2514\u2500\u2500 val/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre>"},{"location":"formats/object-detection/yolov12/#pattern-2-trainval-as-root-directories","title":"Pattern 2: Train/Val as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image3.jpg\n    \u2502   \u2514\u2500\u2500 image4.jpg\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre> <p>The corresponding <code>data.yaml</code> configuration should match your chosen structure:</p> <pre><code># For Pattern 1:\npath: .  # Optional - defaults to current directory if omitted\ntrain: images/train  # Path to training images\nval: images/val      # Path to validation images\n\n# For Pattern 2:\npath: .  # Optional - defaults to current directory if omitted\ntrain: train/images  # Path to training images\nval: val/images      # Path to validation images\n</code></pre> <p>Important: Label files must have the same name as their corresponding image files (excluding the file extension) and must maintain the parallel directory structure, only replacing <code>images</code> with <code>labels</code> in the path.</p>"},{"location":"formats/object-detection/yolov12/#benefits-of-yolov12-format","title":"Benefits of YOLOv12 Format","text":"<ul> <li>Simplicity: Easy to read and write, facilitating quick dataset preparation.</li> <li>Efficiency: Compact representation reduces storage requirements.</li> <li>Compatibility: Maintains consistency across YOLO versions, ensuring seamless integration with various tools and frameworks.</li> </ul>"},{"location":"formats/object-detection/yolov12/#example-of-yolov12-format","title":"Example of YOLOv12 Format","text":"<p>Example of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory (defaults to current directory if omitted). Can also be `../dataset`.\ntrain: images/train  # Directory for training images\nval: images/val  # Directory for validation images\ntest: images/test  # Directory for test images (optional)\n\nnames:\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Example Annotation</p> <p>For an image named <code>image1.jpg</code>, the corresponding <code>image1.txt</code> might contain: <pre><code>0 0.716797 0.395833 0.216406 0.147222\n1 0.687500 0.379167 0.255208 0.175000\n</code></pre></p> <p>Explanation: - The first line represents an object of class <code>0</code> (e.g., <code>cat</code>) with its bounding box centered at <code>(0.716797, 0.395833)</code> relative to the image dimensions, and a width and height of <code>0.216406</code> and <code>0.147222</code> respectively. - The second line represents an object of class <code>1</code> (e.g., <code>dog</code>) with its own bounding box specifications.</p>"},{"location":"formats/object-detection/yolov12/#normalizing-bounding-box-coordinates-for-yolov12","title":"Normalizing Bounding Box Coordinates for YOLOv12","text":"<p>To convert pixel values to normalized values required by YOLOv12:</p> <pre><code># Given pixel values and image dimensions\nx_top_left = 150     # x coordinate of top-left corner of bounding box\ny_top_left = 200     # y coordinate of top-left corner of bounding box\nwidth_pixel = 50     # width of bounding box\nheight_pixel = 80    # height of bounding box\nimage_width = 640\nimage_height = 480\n\n# 1. Convert top-left coordinates to center coordinates (in pixels)\nx_center_pixel = x_top_left + (width_pixel / 2)   # 150 + (50/2) = 175\ny_center_pixel = y_top_left + (height_pixel / 2)  # 200 + (80/2) = 240\n\n# 2. Normalize all values (divide by image dimensions)\nx_center = x_center_pixel / image_width     # 175 / 640 = 0.273438\ny_center = y_center_pixel / image_height    # 240 / 480 = 0.500000\nwidth = width_pixel / image_width           # 50 / 640 = 0.078125\nheight = height_pixel / image_height        # 80 / 480 = 0.166667\n\n# Annotation line format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\nannotation = f\"0 {x_center} {y_center} {width} {height}\"\n# Output: \"0 0.273438 0.500000 0.078125 0.166667\"\n</code></pre>"},{"location":"formats/object-detection/yolov12/#converting-annotations-to-yolov12-format-with-labelformat","title":"Converting Annotations to YOLOv12 Format with Labelformat","text":"<p>Our Labelformat framework simplifies the process of converting various annotation formats to the YOLOv12 detection format. Below is a step-by-step guide to perform this conversion.</p>"},{"location":"formats/object-detection/yolov12/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed. You can install it via pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/yolov12/#conversion-example-coco-to-yolov12","title":"Conversion Example: COCO to YOLOv12","text":"<p>Assume you have annotations in the COCO format and wish to convert them to YOLOv12. Here's how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard COCO structure:</p> <ul> <li>You have a <code>.json</code> file with the COCO annotations. (e.g. <code>annotations/instances_train.json</code>)</li> <li>You have a directory with the images. (e.g. <code>images/</code>)</li> </ul> <p>Full example: <pre><code>dataset/\n\u251c\u2500\u2500 annotations/\n\u2502   \u2514\u2500\u2500 instances_train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n</code></pre></p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert COCO annotations to YOLOv12: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov12 \\\n    --output-folder dataset/yolov12_labels \\\n    --output-split train\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 yolov12_labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n</code></pre></p> <p>Contents of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory\ntrain: images  # Directory for training images\nnc: 3  # Number of classes\nnames:  # Class name mapping\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Contents of <code>image1.txt</code>: <pre><code>0 0.234375 0.416667 0.078125 0.166667\n1 0.500000 0.500000 0.100000 0.200000\n</code></pre></p>"},{"location":"formats/object-detection/yolov12/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>The format implementation includes several safeguards:</p> <ul> <li>Missing Label Files:</li> <li>Warning is logged if a label file doesn't exist for an image</li> <li> <p>Image is skipped from processing</p> </li> <li> <p>File Access Issues:</p> </li> <li>Errors are logged if label files cannot be read due to permissions or other OS issues</li> <li> <p>Affected images are skipped from processing</p> </li> <li> <p>Label Format Validation:</p> </li> <li>Each line must contain exactly 5 space-separated values</li> <li>Invalid lines are logged as warnings and skipped</li> <li>All values must be convertible to appropriate types:<ul> <li>Category ID must be a valid integer</li> <li>Coordinates and dimensions must be valid floats</li> </ul> </li> <li>Category IDs must exist in the category mapping</li> </ul> <p>Example of a properly formatted label file: <pre><code>0 0.716797 0.395833 0.216406 0.147222  # Each line must have exactly 5 space-separated values\n1 0.687500 0.379167 0.255208 0.175000  # All values must be valid numbers within [0,1] range\n</code></pre></p>"},{"location":"formats/object-detection/yolov26/","title":"YOLOv26 (YOLO26) Object Detection Format","text":""},{"location":"formats/object-detection/yolov26/#overview","title":"Overview","text":"<p>YOLOv26 (also known as YOLO26) is the latest evolution in the You Only Look Once (YOLO) series, engineered specifically for edge and low-power devices. It introduces a streamlined design that removes unnecessary complexity while integrating targeted innovations to deliver faster, lighter, and more accessible deployment. YOLOv26 uses the same object detection format as YOLOv8-v12, utilizing normalized coordinates in text files for seamless compatibility.</p> <p>Info: YOLOv26 is currently in preview and under development. Performance numbers are preliminary and final releases will follow soon. For the latest updates, see: GitHub Repository: ultralytics/ultralytics</p>"},{"location":"formats/object-detection/yolov26/#key-yolov26-features","title":"Key YOLOv26 Features","text":"<p>YOLOv26 maintains full compatibility with the YOLOv8-v12 label format while introducing several breakthrough innovations:</p> <ul> <li>End-to-End NMS-Free Inference: Native end-to-end model producing predictions directly without non-maximum suppression, reducing latency and simplifying deployment</li> <li>DFL Removal: Eliminates Distribution Focal Loss module for better export compatibility and broader hardware support on edge devices</li> <li>MuSGD Optimizer: Hybrid optimizer combining SGD with Muon, inspired by Moonshot AI's Kimi K2 breakthroughs in LLM training</li> <li>ProgLoss + STAL: Enhanced loss functions with notable improvements in small-object detection accuracy</li> <li>43% Faster CPU Inference: Specifically optimized for edge computing with significant CPU performance gains</li> </ul>"},{"location":"formats/object-detection/yolov26/#format-specification","title":"Format Specification","text":"<p>YOLOv26 uses the identical format as YOLOv8, YOLOv9, YOLOv10, YOLOv11, and YOLOv12. Please refer to the YOLOv8 format documentation for complete format specifications, including:</p> <ul> <li>Text file structure with normalized coordinates</li> <li>Directory organization patterns</li> <li>Configuration via <code>data.yaml</code></li> <li>Coordinate normalization formulas</li> <li>Example annotations</li> </ul>"},{"location":"formats/object-detection/yolov26/#converting-annotations-to-yolov26-format","title":"Converting Annotations to YOLOv26 Format","text":"<p>Since YOLOv26 uses the same format as YOLOv8-v11, you can convert from other formats using Labelformat:</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov26 \\\n    --output-folder dataset/yolov26_labels \\\n    --output-split train\n</code></pre> <p>The converted output will be fully compatible with YOLOv26 training and inference pipelines.</p>"},{"location":"formats/object-detection/yolov5/","title":"YOLOv5 Object Detection Format","text":""},{"location":"formats/object-detection/yolov5/#overview","title":"Overview","text":"<p>YOLOv5 is a well-known object detection model in the You Only Look Once (YOLO) series, renowned for its real-time object detection capabilities. Building upon the foundations of YOLO series, YOLOv5 introduces significant advancements in model architecture and training methodologies, enhancing both accuracy and efficiency. Despite these improvements, YOLOv5 retains the same object detection format as its predecessors, utilizing normalized coordinates in text files. This consistency ensures seamless integration and compatibility with existing workflows while benefiting from YOLOv5's enhanced performance.</p> <p>Info: Unlike other YOLO versions, YOLOv5 was not introduced through an academic paper. The most commonly referenced implementation is the Ultralytics YOLOv5 repository on GitHub, which has become the de facto standard implementation.    For implementation details and specifications, see: GitHub Repository: ultralytics/yolov5</p>"},{"location":"formats/object-detection/yolov5/#specification-of-yolov5-detection-format","title":"Specification of YOLOv5 Detection Format","text":"<p>The YOLOv5 detection format remains consistent with previous versions, ensuring ease of adoption and compatibility. Below are the detailed specifications:</p> <ul> <li> <p>One Text File per Image:   For every image in your dataset, there exists a corresponding <code>.txt</code> file containing annotation data.</p> </li> <li> <p>Object Representation:   Each line in the text file represents a single object detected within the image, following the format: <code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> </p> <ul> <li><code>&lt;class_id&gt;</code> (Integer):   An integer representing the object's class.</li> <li><code>&lt;x_center&gt;</code> and <code>&lt;y_center&gt;</code> (Float): The normalized coordinates of the object's center relative to the image's width and    height.</li> <li><code>&lt;width&gt;</code> and <code>&lt;height&gt;</code> (Float): The normalized width and height of the bounding box encompassing the object.</li> </ul> </li> <li> <p>Normalization of Values:   All coordinate and size values are normalized to a range between <code>0.0</code> and <code>1.0</code>.  </p> </li> <li> <p>Normalization Formula:     To convert pixel values to normalized coordinates: <pre><code>normalized_x = x_pixel / image_width  \nnormalized_y = y_pixel / image_height  \nnormalized_width = box_width_pixel / image_width  \nnormalized_height = box_height_pixel / image_height\n</code></pre></p> </li> <li> <p>Class ID Indexing:   Class IDs start from <code>0</code>, with each ID corresponding to a specific object category defined in the <code>data.yaml</code> configuration file. Class IDs must be contiguous integers (e.g., 0,1,2 and not 0,2,3) to ensure proper model training and inference.</p> </li> <li> <p>Configuration via <code>data.yaml</code>:   The <code>data.yaml</code> file contains essential configuration settings, including paths to training and validation datasets, number of classes (<code>nc</code>), and a mapping of class names to their respective IDs (<code>names</code>).</p> </li> </ul>"},{"location":"formats/object-detection/yolov5/#directory-structure-requirements","title":"Directory Structure Requirements","text":"<p>The dataset must maintain a parallel directory structure for images and their corresponding label files. There are two common organizational patterns:</p>"},{"location":"formats/object-detection/yolov5/#pattern-1-images-and-labels-as-root-directories","title":"Pattern 1: Images and Labels as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 image3.jpg\n\u2502       \u2514\u2500\u2500 image4.jpg\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 image1.txt\n    \u2502   \u2514\u2500\u2500 image2.txt\n    \u2514\u2500\u2500 val/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre>"},{"location":"formats/object-detection/yolov5/#pattern-2-trainval-as-root-directories","title":"Pattern 2: Train/Val as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image3.jpg\n    \u2502   \u2514\u2500\u2500 image4.jpg\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre> <p>The corresponding <code>data.yaml</code> configuration should match your chosen structure:</p> <pre><code># For Pattern 1:\npath: .  # Optional - defaults to current directory if omitted\ntrain: images/train  # Path to training images\nval: images/val      # Path to validation images\n\n# For Pattern 2:\npath: .  # Optional - defaults to current directory if omitted\ntrain: train/images  # Path to training images\nval: val/images      # Path to validation images\n</code></pre> <p>Important: Label files must have the same name as their corresponding image files (excluding the file extension) and must maintain the parallel directory structure, only replacing <code>images</code> with <code>labels</code> in the path. </p>"},{"location":"formats/object-detection/yolov5/#benefits-of-yolov5-format","title":"Benefits of YOLOv5 Format","text":"<ul> <li>Simplicity: Easy to read and write, facilitating quick dataset preparation.</li> <li>Efficiency: Compact representation reduces storage requirements.</li> <li>Compatibility: Maintains consistency across YOLO versions, ensuring seamless integration with various tools and frameworks.</li> </ul>"},{"location":"formats/object-detection/yolov5/#example-of-yolov5-format","title":"Example of YOLOv5 Format","text":"<p>Example of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory (defaults to current directory if omitted). Can also be `../dataset`.\ntrain: images/train  # Directory for training images\nval: images/val  # Directory for validation images\ntest: images/test  # Directory for test images (optional)\n\nnames:\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Example Annotation</p> <p>For an image named <code>image1.jpg</code>, the corresponding <code>image1.txt</code> might contain: <pre><code>0 0.716797 0.395833 0.216406 0.147222\n1 0.687500 0.379167 0.255208 0.175000\n</code></pre></p> <p>Explanation: - The first line represents an object of class <code>0</code> (e.g., <code>cat</code>) with its bounding box centered at <code>(0.716797, 0.395833)</code> relative to the image dimensions, and a width and height of <code>0.216406</code> and <code>0.147222</code> respectively. - The second line represents an object of class <code>1</code> (e.g., <code>dog</code>) with its own bounding box specifications.</p>"},{"location":"formats/object-detection/yolov5/#normalizing-bounding-box-coordinates-for-yolov5","title":"Normalizing Bounding Box Coordinates for YOLOv5","text":"<p>To convert pixel values to normalized values required by YOLOv5:</p> <pre><code># Given pixel values and image dimensions\nx_top_left = 150     # x coordinate of top-left corner of bounding box\ny_top_left = 200     # y coordinate of top-left corner of bounding box\nwidth_pixel = 50     # width of bounding box\nheight_pixel = 80    # height of bounding box\nimage_width = 640\nimage_height = 480\n\n# 1. Convert top-left coordinates to center coordinates (in pixels)\nx_center_pixel = x_top_left + (width_pixel / 2)   # 150 + (50/2) = 175\ny_center_pixel = y_top_left + (height_pixel / 2)  # 200 + (80/2) = 240\n\n# 2. Normalize all values (divide by image dimensions)\nx_center = x_center_pixel / image_width     # 175 / 640 = 0.273438\ny_center = y_center_pixel / image_height    # 240 / 480 = 0.500000\nwidth = width_pixel / image_width           # 50 / 640 = 0.078125\nheight = height_pixel / image_height        # 80 / 480 = 0.166667\n\n# Annotation line format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\nannotation = f\"0 {x_center} {y_center} {width} {height}\"\n# Output: \"0 0.273438 0.500000 0.078125 0.166667\"\n</code></pre>"},{"location":"formats/object-detection/yolov5/#converting-annotations-to-yolov5-format-with-labelformat","title":"Converting Annotations to YOLOv5 Format with Labelformat","text":"<p>Our Labelformat framework simplifies the process of converting various annotation formats to the YOLOv5 detection format. Below is a step-by-step guide to perform this conversion.</p>"},{"location":"formats/object-detection/yolov5/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed. You can install it via pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/yolov5/#conversion-example-coco-to-yolov5","title":"Conversion Example: COCO to YOLOv5","text":"<p>Assume you have annotations in the COCO format and wish to convert them to YOLOv5. Here\u2019s how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard COCO structure:</p> <ul> <li>You have a <code>.json</code> file with the COCO annotations. (e.g. <code>annotations/instances_train.json</code>)</li> <li>You have a directory with the images. (e.g. <code>images/</code>)</li> </ul> <p>Full example: <pre><code>dataset/\n\u251c\u2500\u2500 annotations/\n\u2502   \u2514\u2500\u2500 instances_train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n</code></pre></p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert COCO annotations to YOLOv5: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov5 \\\n    --output-folder dataset/yolov5_labels \\\n    --output-split train\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 yolov5_labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n</code></pre></p> <p>Contents of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory\ntrain: images  # Directory for training images\nnc: 3  # Number of classes\nnames:  # Class name mapping\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Contents of <code>image1.txt</code>: <pre><code>0 0.234375 0.416667 0.078125 0.166667\n1 0.500000 0.500000 0.100000 0.200000\n</code></pre></p>"},{"location":"formats/object-detection/yolov5/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>The format implementation includes several safeguards:</p> <ul> <li>Missing Label Files:</li> <li>Warning is logged if a label file doesn't exist for an image</li> <li> <p>Image is skipped from processing</p> </li> <li> <p>File Access Issues:</p> </li> <li>Errors are logged if label files cannot be read due to permissions or other OS issues</li> <li> <p>Affected images are skipped from processing</p> </li> <li> <p>Label Format Validation:</p> </li> <li>Each line must contain exactly 5 space-separated values</li> <li>Invalid lines are logged as warnings and skipped</li> <li>All values must be convertible to appropriate types:<ul> <li>Category ID must be a valid integer</li> <li>Coordinates and dimensions must be valid floats</li> </ul> </li> <li>Category IDs must exist in the category mapping</li> </ul> <p>Example of a properly formatted label file: <pre><code>0 0.716797 0.395833 0.216406 0.147222  # Each line must have exactly 5 space-separated values\n1 0.687500 0.379167 0.255208 0.175000  # All values must be valid numbers within [0,1] range\n</code></pre></p>"},{"location":"formats/object-detection/yolov6/","title":"YOLOv6 Object Detection Format","text":""},{"location":"formats/object-detection/yolov6/#overview","title":"Overview","text":"<p>YOLOv6 is a well-known object detection model in the You Only Look Once (YOLO) series, introduced in the paper YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications. Building upon the foundations of YOLOv5, YOLOv6 introduces significant advancements in model architecture and training methodologies, enhancing both accuracy and efficiency. Despite these improvements, YOLOv6 retains the same object detection format as its predecessors, utilizing normalized coordinates in text files. This consistency ensures seamless integration and compatibility with existing workflows while benefiting from YOLOv6's enhanced performance.</p>"},{"location":"formats/object-detection/yolov6/#specification-of-yolov6-detection-format","title":"Specification of YOLOv6 Detection Format","text":"<p>The YOLOv6 detection format remains consistent with previous versions (such as YOLOv5), ensuring ease of adoption and compatibility. Below are the detailed specifications:</p> <ul> <li> <p>One Text File per Image:   For every image in your dataset, there exists a corresponding <code>.txt</code> file containing annotation data.</p> </li> <li> <p>Object Representation:   Each line in the text file represents a single object detected within the image, following the format: <code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> </p> <ul> <li><code>&lt;class_id&gt;</code> (Integer):   An integer representing the object's class.</li> <li><code>&lt;x_center&gt;</code> and <code>&lt;y_center&gt;</code> (Float): The normalized coordinates of the object's center relative to the image's width and    height.</li> <li><code>&lt;width&gt;</code> and <code>&lt;height&gt;</code> (Float): The normalized width and height of the bounding box encompassing the object.</li> </ul> </li> <li> <p>Normalization of Values:   All coordinate and size values are normalized to a range between <code>0.0</code> and <code>1.0</code>.  </p> </li> <li> <p>Normalization Formula:     To convert pixel values to normalized coordinates: <pre><code>normalized_x = x_pixel / image_width  \nnormalized_y = y_pixel / image_height  \nnormalized_width = box_width_pixel / image_width  \nnormalized_height = box_height_pixel / image_height\n</code></pre></p> </li> <li> <p>Class ID Indexing:   Class IDs start from <code>0</code>, with each ID corresponding to a specific object category defined in the <code>data.yaml</code> configuration file. Class IDs must be contiguous integers (e.g., 0,1,2 and not 0,2,3) to ensure proper model training and inference.</p> </li> <li> <p>Configuration via <code>data.yaml</code>:   The <code>data.yaml</code> file contains essential configuration settings, including paths to training and validation datasets, number of classes (<code>nc</code>), and a mapping of class names to their respective IDs (<code>names</code>).</p> </li> </ul>"},{"location":"formats/object-detection/yolov6/#directory-structure-requirements","title":"Directory Structure Requirements","text":"<p>The dataset must maintain a parallel directory structure for images and their corresponding label files. There are two common organizational patterns:</p>"},{"location":"formats/object-detection/yolov6/#pattern-1-images-and-labels-as-root-directories","title":"Pattern 1: Images and Labels as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 image3.jpg\n\u2502       \u2514\u2500\u2500 image4.jpg\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 image1.txt\n    \u2502   \u2514\u2500\u2500 image2.txt\n    \u2514\u2500\u2500 val/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre>"},{"location":"formats/object-detection/yolov6/#pattern-2-trainval-as-root-directories","title":"Pattern 2: Train/Val as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image3.jpg\n    \u2502   \u2514\u2500\u2500 image4.jpg\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre> <p>The corresponding <code>data.yaml</code> configuration should match your chosen structure:</p> <pre><code># For Pattern 1:\npath: .  # Optional - defaults to current directory if omitted\ntrain: images/train  # Path to training images\nval: images/val      # Path to validation images\n\n# For Pattern 2:\npath: .  # Optional - defaults to current directory if omitted\ntrain: train/images  # Path to training images\nval: val/images      # Path to validation images\n</code></pre> <p>Important: Label files must have the same name as their corresponding image files (excluding the file extension) and must maintain the parallel directory structure, only replacing <code>images</code> with <code>labels</code> in the path. </p>"},{"location":"formats/object-detection/yolov6/#benefits-of-yolov6-format","title":"Benefits of YOLOv6 Format","text":"<ul> <li>Simplicity: Easy to read and write, facilitating quick dataset preparation.</li> <li>Efficiency: Compact representation reduces storage requirements.</li> <li>Compatibility: Maintains consistency across YOLO versions, ensuring seamless integration with various tools and frameworks.</li> </ul>"},{"location":"formats/object-detection/yolov6/#example-of-yolov6-format","title":"Example of YOLOv6 Format","text":"<p>Example of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory (defaults to current directory if omitted). Can also be `../dataset`.\ntrain: images/train  # Directory for training images\nval: images/val  # Directory for validation images\ntest: images/test  # Directory for test images (optional)\n\nnames:\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Example Annotation</p> <p>For an image named <code>image1.jpg</code>, the corresponding <code>image1.txt</code> might contain: <pre><code>0 0.716797 0.395833 0.216406 0.147222\n1 0.687500 0.379167 0.255208 0.175000\n</code></pre></p> <p>Explanation: - The first line represents an object of class <code>0</code> (e.g., <code>cat</code>) with its bounding box centered at <code>(0.716797, 0.395833)</code> relative to the image dimensions, and a width and height of <code>0.216406</code> and <code>0.147222</code> respectively. - The second line represents an object of class <code>1</code> (e.g., <code>dog</code>) with its own bounding box specifications.</p>"},{"location":"formats/object-detection/yolov6/#normalizing-bounding-box-coordinates-for-yolov6","title":"Normalizing Bounding Box Coordinates for YOLOv6","text":"<p>To convert pixel values to normalized values required by YOLOv6:</p> <pre><code># Given pixel values and image dimensions\nx_top_left = 150     # x coordinate of top-left corner of bounding box\ny_top_left = 200     # y coordinate of top-left corner of bounding box\nwidth_pixel = 50     # width of bounding box\nheight_pixel = 80    # height of bounding box\nimage_width = 640\nimage_height = 480\n\n# 1. Convert top-left coordinates to center coordinates (in pixels)\nx_center_pixel = x_top_left + (width_pixel / 2)   # 150 + (50/2) = 175\ny_center_pixel = y_top_left + (height_pixel / 2)  # 200 + (80/2) = 240\n\n# 2. Normalize all values (divide by image dimensions)\nx_center = x_center_pixel / image_width     # 175 / 640 = 0.273438\ny_center = y_center_pixel / image_height    # 240 / 480 = 0.500000\nwidth = width_pixel / image_width           # 50 / 640 = 0.078125\nheight = height_pixel / image_height        # 80 / 480 = 0.166667\n\n# Annotation line format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\nannotation = f\"0 {x_center} {y_center} {width} {height}\"\n# Output: \"0 0.273438 0.500000 0.078125 0.166667\"\n</code></pre>"},{"location":"formats/object-detection/yolov6/#converting-annotations-to-yolov6-format-with-labelformat","title":"Converting Annotations to YOLOv6 Format with Labelformat","text":"<p>Our Labelformat framework simplifies the process of converting various annotation formats to the YOLOv6 detection format. Below is a step-by-step guide to perform this conversion.</p>"},{"location":"formats/object-detection/yolov6/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed. You can install it via pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/yolov6/#conversion-example-coco-to-yolov6","title":"Conversion Example: COCO to YOLOv6","text":"<p>Assume you have annotations in the COCO format and wish to convert them to YOLOv6. Here\u2019s how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard COCO structure:</p> <ul> <li>You have a <code>.json</code> file with the COCO annotations. (e.g. <code>annotations/instances_train.json</code>)</li> <li>You have a directory with the images. (e.g. <code>images/</code>)</li> </ul> <p>Full example: <pre><code>dataset/\n\u251c\u2500\u2500 annotations/\n\u2502   \u2514\u2500\u2500 instances_train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n</code></pre></p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert COCO annotations to YOLOv6: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov6 \\\n    --output-folder dataset/yolov6_labels \\\n    --output-split train\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 yolov6_labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n</code></pre></p> <p>Contents of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory\ntrain: images  # Directory for training images\nnc: 3  # Number of classes\nnames:  # Class name mapping\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Contents of <code>image1.txt</code>: <pre><code>0 0.234375 0.416667 0.078125 0.166667\n1 0.500000 0.500000 0.100000 0.200000\n</code></pre></p>"},{"location":"formats/object-detection/yolov6/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>The format implementation includes several safeguards:</p> <ul> <li>Missing Label Files:</li> <li>Warning is logged if a label file doesn't exist for an image</li> <li> <p>Image is skipped from processing</p> </li> <li> <p>File Access Issues:</p> </li> <li>Errors are logged if label files cannot be read due to permissions or other OS issues</li> <li> <p>Affected images are skipped from processing</p> </li> <li> <p>Label Format Validation:</p> </li> <li>Each line must contain exactly 5 space-separated values</li> <li>Invalid lines are logged as warnings and skipped</li> <li>All values must be convertible to appropriate types:<ul> <li>Category ID must be a valid integer</li> <li>Coordinates and dimensions must be valid floats</li> </ul> </li> <li>Category IDs must exist in the category mapping</li> </ul> <p>Example of a properly formatted label file: <pre><code>0 0.716797 0.395833 0.216406 0.147222  # Each line must have exactly 5 space-separated values\n1 0.687500 0.379167 0.255208 0.175000  # All values must be valid numbers within [0,1] range\n</code></pre></p>"},{"location":"formats/object-detection/yolov7/","title":"YOLOv7 Object Detection Format","text":""},{"location":"formats/object-detection/yolov7/#overview","title":"Overview","text":"<p>YOLOv7 is a well-known object detection model in the You Only Look Once (YOLO) series, introduced in the paper YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. Building upon the foundations of YOLOv5 through YOLOv6, YOLOv7 introduces significant advancements in model architecture and training methodologies, enhancing both accuracy and efficiency. Despite these improvements, YOLOv7 retains the same object detection format as its predecessors, utilizing normalized coordinates in text files. This consistency ensures seamless integration and compatibility with existing workflows while benefiting from YOLOv7's enhanced performance.</p>"},{"location":"formats/object-detection/yolov7/#specification-of-yolov7-detection-format","title":"Specification of YOLOv7 Detection Format","text":"<p>The YOLOv7 detection format remains consistent with previous versions (v5-v6), ensuring ease of adoption and compatibility. Below are the detailed specifications:</p> <ul> <li> <p>One Text File per Image:   For every image in your dataset, there exists a corresponding <code>.txt</code> file containing annotation data.</p> </li> <li> <p>Object Representation:   Each line in the text file represents a single object detected within the image, following the format: <code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> </p> <ul> <li><code>&lt;class_id&gt;</code> (Integer):   An integer representing the object's class.</li> <li><code>&lt;x_center&gt;</code> and <code>&lt;y_center&gt;</code> (Float): The normalized coordinates of the object's center relative to the image's width and    height.</li> <li><code>&lt;width&gt;</code> and <code>&lt;height&gt;</code> (Float): The normalized width and height of the bounding box encompassing the object.</li> </ul> </li> <li> <p>Normalization of Values:   All coordinate and size values are normalized to a range between <code>0.0</code> and <code>1.0</code>.  </p> </li> <li> <p>Normalization Formula:     To convert pixel values to normalized coordinates: <pre><code>normalized_x = x_pixel / image_width  \nnormalized_y = y_pixel / image_height  \nnormalized_width = box_width_pixel / image_width  \nnormalized_height = box_height_pixel / image_height\n</code></pre></p> </li> <li> <p>Class ID Indexing:   Class IDs start from <code>0</code>, with each ID corresponding to a specific object category defined in the <code>data.yaml</code> configuration file. Class IDs must be contiguous integers (e.g., 0,1,2 and not 0,2,3) to ensure proper model training and inference.</p> </li> <li> <p>Configuration via <code>data.yaml</code>:   The <code>data.yaml</code> file contains essential configuration settings, including paths to training and validation datasets, number of classes (<code>nc</code>), and a mapping of class names to their respective IDs (<code>names</code>).</p> </li> </ul>"},{"location":"formats/object-detection/yolov7/#directory-structure-requirements","title":"Directory Structure Requirements","text":"<p>The dataset must maintain a parallel directory structure for images and their corresponding label files. There are two common organizational patterns:</p>"},{"location":"formats/object-detection/yolov7/#pattern-1-images-and-labels-as-root-directories","title":"Pattern 1: Images and Labels as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 image3.jpg\n\u2502       \u2514\u2500\u2500 image4.jpg\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 image1.txt\n    \u2502   \u2514\u2500\u2500 image2.txt\n    \u2514\u2500\u2500 val/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre>"},{"location":"formats/object-detection/yolov7/#pattern-2-trainval-as-root-directories","title":"Pattern 2: Train/Val as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image3.jpg\n    \u2502   \u2514\u2500\u2500 image4.jpg\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre> <p>The corresponding <code>data.yaml</code> configuration should match your chosen structure:</p> <pre><code># For Pattern 1:\npath: .  # Optional - defaults to current directory if omitted\ntrain: images/train  # Path to training images\nval: images/val      # Path to validation images\n\n# For Pattern 2:\npath: .  # Optional - defaults to current directory if omitted\ntrain: train/images  # Path to training images\nval: val/images      # Path to validation images\n</code></pre> <p>Important: Label files must have the same name as their corresponding image files (excluding the file extension) and must maintain the parallel directory structure, only replacing <code>images</code> with <code>labels</code> in the path. </p>"},{"location":"formats/object-detection/yolov7/#benefits-of-yolov7-format","title":"Benefits of YOLOv7 Format","text":"<ul> <li>Simplicity: Easy to read and write, facilitating quick dataset preparation.</li> <li>Efficiency: Compact representation reduces storage requirements.</li> <li>Compatibility: Maintains consistency across YOLO versions, ensuring seamless integration with various tools and frameworks.</li> </ul>"},{"location":"formats/object-detection/yolov7/#example-of-yolov7-format","title":"Example of YOLOv7 Format","text":"<p>Example of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory (defaults to current directory if omitted). Can also be `../dataset`.\ntrain: images/train  # Directory for training images\nval: images/val  # Directory for validation images\ntest: images/test  # Directory for test images (optional)\n\nnames:\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Example Annotation</p> <p>For an image named <code>image1.jpg</code>, the corresponding <code>image1.txt</code> might contain: <pre><code>0 0.716797 0.395833 0.216406 0.147222\n1 0.687500 0.379167 0.255208 0.175000\n</code></pre></p> <p>Explanation: - The first line represents an object of class <code>0</code> (e.g., <code>cat</code>) with its bounding box centered at <code>(0.716797, 0.395833)</code> relative to the image dimensions, and a width and height of <code>0.216406</code> and <code>0.147222</code> respectively. - The second line represents an object of class <code>1</code> (e.g., <code>dog</code>) with its own bounding box specifications.</p>"},{"location":"formats/object-detection/yolov7/#normalizing-bounding-box-coordinates-for-yolov7","title":"Normalizing Bounding Box Coordinates for YOLOv7","text":"<p>To convert pixel values to normalized values required by YOLOv7:</p> <pre><code># Given pixel values and image dimensions\nx_top_left = 150     # x coordinate of top-left corner of bounding box\ny_top_left = 200     # y coordinate of top-left corner of bounding box\nwidth_pixel = 50     # width of bounding box\nheight_pixel = 80    # height of bounding box\nimage_width = 640\nimage_height = 480\n\n# 1. Convert top-left coordinates to center coordinates (in pixels)\nx_center_pixel = x_top_left + (width_pixel / 2)   # 150 + (50/2) = 175\ny_center_pixel = y_top_left + (height_pixel / 2)  # 200 + (80/2) = 240\n\n# 2. Normalize all values (divide by image dimensions)\nx_center = x_center_pixel / image_width     # 175 / 640 = 0.273438\ny_center = y_center_pixel / image_height    # 240 / 480 = 0.500000\nwidth = width_pixel / image_width           # 50 / 640 = 0.078125\nheight = height_pixel / image_height        # 80 / 480 = 0.166667\n\n# Annotation line format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\nannotation = f\"0 {x_center} {y_center} {width} {height}\"\n# Output: \"0 0.273438 0.500000 0.078125 0.166667\"\n</code></pre>"},{"location":"formats/object-detection/yolov7/#converting-annotations-to-yolov7-format-with-labelformat","title":"Converting Annotations to YOLOv7 Format with Labelformat","text":"<p>Our Labelformat framework simplifies the process of converting various annotation formats to the YOLOv7 detection format. Below is a step-by-step guide to perform this conversion.</p>"},{"location":"formats/object-detection/yolov7/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed. You can install it via pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/yolov7/#conversion-example-coco-to-yolov7","title":"Conversion Example: COCO to YOLOv7","text":"<p>Assume you have annotations in the COCO format and wish to convert them to YOLOv7. Here\u2019s how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard COCO structure:</p> <ul> <li>You have a <code>.json</code> file with the COCO annotations. (e.g. <code>annotations/instances_train.json</code>)</li> <li>You have a directory with the images. (e.g. <code>images/</code>)</li> </ul> <p>Full example: <pre><code>dataset/\n\u251c\u2500\u2500 annotations/\n\u2502   \u2514\u2500\u2500 instances_train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n</code></pre></p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert COCO annotations to YOLOv7: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov7 \\\n    --output-folder dataset/yolov7_labels \\\n    --output-split train\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 yolov7_labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n</code></pre></p> <p>Contents of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory\ntrain: images  # Directory for training images\nnc: 3  # Number of classes\nnames:  # Class name mapping\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Contents of <code>image1.txt</code>: <pre><code>0 0.234375 0.416667 0.078125 0.166667\n1 0.500000 0.500000 0.100000 0.200000\n</code></pre></p>"},{"location":"formats/object-detection/yolov7/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>The format implementation includes several safeguards:</p> <ul> <li>Missing Label Files:</li> <li>Warning is logged if a label file doesn't exist for an image</li> <li> <p>Image is skipped from processing</p> </li> <li> <p>File Access Issues:</p> </li> <li>Errors are logged if label files cannot be read due to permissions or other OS issues</li> <li> <p>Affected images are skipped from processing</p> </li> <li> <p>Label Format Validation:</p> </li> <li>Each line must contain exactly 5 space-separated values</li> <li>Invalid lines are logged as warnings and skipped</li> <li>All values must be convertible to appropriate types:<ul> <li>Category ID must be a valid integer</li> <li>Coordinates and dimensions must be valid floats</li> </ul> </li> <li>Category IDs must exist in the category mapping</li> </ul> <p>Example of a properly formatted label file: <pre><code>0 0.716797 0.395833 0.216406 0.147222  # Each line must have exactly 5 space-separated values\n1 0.687500 0.379167 0.255208 0.175000  # All values must be valid numbers within [0,1] range\n</code></pre></p>"},{"location":"formats/object-detection/yolov8/","title":"YOLOv8 Object Detection Format","text":""},{"location":"formats/object-detection/yolov8/#overview","title":"Overview","text":"<p>YOLOv8 is a well-known object detection model in the You Only Look Once (YOLO) series, renowned for its real-time object detection capabilities. Building upon the foundations of YOLOv5 through YOLOv7, YOLOv8 introduces significant advancements in model architecture and training methodologies, enhancing both accuracy and efficiency. Despite these improvements, YOLOv8 retains the same object detection format as its predecessors, utilizing normalized coordinates in text files. This consistency ensures seamless integration and compatibility with existing workflows while benefiting from YOLOv8's enhanced performance.</p> <p>Info: Unlike other YOLO versions, YOLOv8 was not introduced through an academic paper. The most commonly referenced implementation is the Ultralytics YOLOv8 repository on GitHub, which has become the de facto standard implementation.    For implementation details and specifications, see: GitHub Repository: ultralytics/yolov8</p>"},{"location":"formats/object-detection/yolov8/#specification-of-yolov8-detection-format","title":"Specification of YOLOv8 Detection Format","text":"<p>The YOLOv8 detection format remains consistent with previous versions (v5-v8), ensuring ease of adoption and compatibility. Below are the detailed specifications:</p> <ul> <li> <p>One Text File per Image:   For every image in your dataset, there exists a corresponding <code>.txt</code> file containing annotation data.</p> </li> <li> <p>Object Representation:   Each line in the text file represents a single object detected within the image, following the format: <code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> </p> <ul> <li><code>&lt;class_id&gt;</code> (Integer):   An integer representing the object's class.</li> <li><code>&lt;x_center&gt;</code> and <code>&lt;y_center&gt;</code> (Float): The normalized coordinates of the object's center relative to the image's width and    height.</li> <li><code>&lt;width&gt;</code> and <code>&lt;height&gt;</code> (Float): The normalized width and height of the bounding box encompassing the object.</li> </ul> </li> <li> <p>Normalization of Values:   All coordinate and size values are normalized to a range between <code>0.0</code> and <code>1.0</code>.  </p> </li> <li> <p>Normalization Formula:     To convert pixel values to normalized coordinates: <pre><code>normalized_x = x_pixel / image_width  \nnormalized_y = y_pixel / image_height  \nnormalized_width = box_width_pixel / image_width  \nnormalized_height = box_height_pixel / image_height\n</code></pre></p> </li> <li> <p>Class ID Indexing:   Class IDs start from <code>0</code>, with each ID corresponding to a specific object category defined in the <code>data.yaml</code> configuration file. Class IDs must be contiguous integers (e.g., 0,1,2 and not 0,2,3) to ensure proper model training and inference.</p> </li> <li> <p>Configuration via <code>data.yaml</code>:   The <code>data.yaml</code> file contains essential configuration settings, including paths to training and validation datasets, number of classes (<code>nc</code>), and a mapping of class names to their respective IDs (<code>names</code>).</p> </li> </ul>"},{"location":"formats/object-detection/yolov8/#directory-structure-of-yolov8-dataset","title":"Directory Structure of YOLOv8 Dataset","text":"<p>The dataset must maintain a parallel directory structure for images and their corresponding label files. There are two common organizational patterns:</p>"},{"location":"formats/object-detection/yolov8/#pattern-1-images-and-labels-as-root-directories","title":"Pattern 1: Images and Labels as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 image3.jpg\n\u2502       \u2514\u2500\u2500 image4.jpg\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 image1.txt\n    \u2502   \u2514\u2500\u2500 image2.txt\n    \u2514\u2500\u2500 val/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre>"},{"location":"formats/object-detection/yolov8/#pattern-2-trainval-as-root-directories","title":"Pattern 2: Train/Val as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image3.jpg\n    \u2502   \u2514\u2500\u2500 image4.jpg\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre> <p>The corresponding <code>data.yaml</code> configuration should match your chosen structure:</p> <pre><code># For Pattern 1:\npath: .  # Optional - defaults to current directory if omitted\ntrain: images/train  # Path to training images\nval: images/val      # Path to validation images\n\n# For Pattern 2:\npath: .  # Optional - defaults to current directory if omitted\ntrain: train/images  # Path to training images\nval: val/images      # Path to validation images\n</code></pre> <p>Important: Label files must have the same name as their corresponding image files (excluding the file extension) and must maintain the parallel directory structure, only replacing <code>images</code> with <code>labels</code> in the path. </p>"},{"location":"formats/object-detection/yolov8/#benefits-of-yolov8-format","title":"Benefits of YOLOv8 Format","text":"<ul> <li>Simplicity: Easy to read and write, facilitating quick dataset preparation.</li> <li>Efficiency: Compact representation reduces storage requirements.</li> <li>Compatibility: Maintains consistency across YOLO versions, ensuring seamless integration with various tools and frameworks.</li> </ul>"},{"location":"formats/object-detection/yolov8/#example-of-yolov8-format","title":"Example of YOLOv8 Format","text":"<p>Example of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory (defaults to current directory if omitted). Can also be `../dataset`.\ntrain: images/train  # Directory for training images\nval: images/val  # Directory for validation images\ntest: images/test  # Directory for test images (optional)\n\nnames:\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Example Annotation</p> <p>For an image named <code>image1.jpg</code>, the corresponding <code>image1.txt</code> might contain: <pre><code>0 0.716797 0.395833 0.216406 0.147222\n1 0.687500 0.379167 0.255208 0.175000\n</code></pre></p> <p>Explanation: - The first line represents an object of class <code>0</code> (e.g., <code>cat</code>) with its bounding box centered at <code>(0.716797, 0.395833)</code> relative to the image dimensions, and a width and height of <code>0.216406</code> and <code>0.147222</code> respectively. - The second line represents an object of class <code>1</code> (e.g., <code>dog</code>) with its own bounding box specifications.</p>"},{"location":"formats/object-detection/yolov8/#normalizing-bounding-box-coordinates-for-yolov8","title":"Normalizing Bounding Box Coordinates for YOLOv8","text":"<p>To convert pixel values to normalized values required by YOLOv8:</p> <pre><code># Given pixel values and image dimensions\nx_top_left = 150     # x coordinate of top-left corner of bounding box\ny_top_left = 200     # y coordinate of top-left corner of bounding box\nwidth_pixel = 50     # width of bounding box\nheight_pixel = 80    # height of bounding box\nimage_width = 640\nimage_height = 480\n\n# 1. Convert top-left coordinates to center coordinates (in pixels)\nx_center_pixel = x_top_left + (width_pixel / 2)   # 150 + (50/2) = 175\ny_center_pixel = y_top_left + (height_pixel / 2)  # 200 + (80/2) = 240\n\n# 2. Normalize all values (divide by image dimensions)\nx_center = x_center_pixel / image_width     # 175 / 640 = 0.273438\ny_center = y_center_pixel / image_height    # 240 / 480 = 0.500000\nwidth = width_pixel / image_width           # 50 / 640 = 0.078125\nheight = height_pixel / image_height        # 80 / 480 = 0.166667\n\n# Annotation line format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\nannotation = f\"0 {x_center} {y_center} {width} {height}\"\n# Output: \"0 0.273438 0.500000 0.078125 0.166667\"\n</code></pre>"},{"location":"formats/object-detection/yolov8/#converting-annotations-to-yolov8-format-with-labelformat","title":"Converting Annotations to YOLOv8 Format with Labelformat","text":"<p>Our Labelformat framework simplifies the process of converting various annotation formats to the YOLOv8 detection format. Below is a step-by-step guide to perform this conversion.</p>"},{"location":"formats/object-detection/yolov8/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed. You can install it via pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/yolov8/#conversion-example-coco-to-yolov8","title":"Conversion Example: COCO to YOLOv8","text":"<p>Assume you have annotations in the COCO format and wish to convert them to YOLOv8. Here\u2019s how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard COCO structure:</p> <ul> <li>You have a <code>.json</code> file with the COCO annotations. (e.g. <code>annotations/instances_train.json</code>)</li> <li>You have a directory with the images. (e.g. <code>images/</code>)</li> </ul> <p>Full example: <pre><code>dataset/\n\u251c\u2500\u2500 annotations/\n\u2502   \u2514\u2500\u2500 instances_train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n</code></pre></p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert COCO annotations to YOLOv8: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov8 \\\n    --output-folder dataset/yolov8_labels \\\n    --output-split train\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 yolov8_labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n</code></pre></p> <p>Contents of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory\ntrain: images  # Directory for training images\nnc: 3  # Number of classes\nnames:  # Class name mapping\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Contents of <code>image1.txt</code>: <pre><code>0 0.234375 0.416667 0.078125 0.166667\n1 0.500000 0.500000 0.100000 0.200000\n</code></pre></p>"},{"location":"formats/object-detection/yolov8/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>The format implementation includes several safeguards:</p> <ul> <li>Missing Label Files:</li> <li>Warning is logged if a label file doesn't exist for an image</li> <li> <p>Image is skipped from processing</p> </li> <li> <p>File Access Issues:</p> </li> <li>Errors are logged if label files cannot be read due to permissions or other OS issues</li> <li> <p>Affected images are skipped from processing</p> </li> <li> <p>Label Format Validation:</p> </li> <li>Each line must contain exactly 5 space-separated values</li> <li>Invalid lines are logged as warnings and skipped</li> <li>All values must be convertible to appropriate types:<ul> <li>Category ID must be a valid integer</li> <li>Coordinates and dimensions must be valid floats</li> </ul> </li> <li>Category IDs must exist in the category mapping</li> </ul> <p>Example of a properly formatted label file: <pre><code>0 0.716797 0.395833 0.216406 0.147222  # Each line must have exactly 5 space-separated values\n1 0.687500 0.379167 0.255208 0.175000  # All values must be valid numbers within [0,1] range\n</code></pre></p>"},{"location":"formats/object-detection/yolov9/","title":"YOLOv9 Object Detection Format","text":""},{"location":"formats/object-detection/yolov9/#overview","title":"Overview","text":"<p>YOLOv9 is a well-known object detection model in the You Only Look Once (YOLO) series, introduced in the paper YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information. Building upon the foundations of YOLOv5 through YOLOv8, YOLOv9 introduces significant advancements in model architecture and training methodologies, enhancing both accuracy and efficiency. Despite these improvements, YOLOv9 retains the same object detection format as its predecessors, utilizing normalized coordinates in text files. This consistency ensures seamless integration and compatibility with existing workflows while benefiting from YOLOv9's enhanced performance.</p>"},{"location":"formats/object-detection/yolov9/#specification-of-yolov9-detection-format","title":"Specification of YOLOv9 Detection Format","text":"<p>The YOLOv9 detection format remains consistent with previous versions (v5-v8), ensuring ease of adoption and compatibility. Below are the detailed specifications:</p> <ul> <li> <p>One Text File per Image:   For every image in your dataset, there exists a corresponding <code>.txt</code> file containing annotation data.</p> </li> <li> <p>Object Representation:   Each line in the text file represents a single object detected within the image, following the format: <code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> </p> <ul> <li><code>&lt;class_id&gt;</code> (Integer):   An integer representing the object's class.</li> <li><code>&lt;x_center&gt;</code> and <code>&lt;y_center&gt;</code> (Float): The normalized coordinates of the object's center relative to the image's width and    height.</li> <li><code>&lt;width&gt;</code> and <code>&lt;height&gt;</code> (Float): The normalized width and height of the bounding box encompassing the object.</li> </ul> </li> <li> <p>Normalization of Values:   All coordinate and size values are normalized to a range between <code>0.0</code> and <code>1.0</code>.  </p> </li> <li> <p>Normalization Formula:     To convert pixel values to normalized coordinates: <pre><code>normalized_x = x_pixel / image_width  \nnormalized_y = y_pixel / image_height  \nnormalized_width = box_width_pixel / image_width  \nnormalized_height = box_height_pixel / image_height\n</code></pre></p> </li> <li> <p>Class ID Indexing:   Class IDs start from <code>0</code>, with each ID corresponding to a specific object category defined in the <code>data.yaml</code> configuration file. Class IDs must be contiguous integers (e.g., 0,1,2 and not 0,2,3) to ensure proper model training and inference.</p> </li> <li> <p>Configuration via <code>data.yaml</code>:   The <code>data.yaml</code> file contains essential configuration settings, including paths to training and validation datasets, number of classes (<code>nc</code>), and a mapping of class names to their respective IDs (<code>names</code>).</p> </li> </ul>"},{"location":"formats/object-detection/yolov9/#directory-structure-of-yolov9-dataset","title":"Directory Structure of YOLOv9 Dataset","text":"<p>The dataset must maintain a parallel directory structure for images and their corresponding label files. There are two common organizational patterns:</p>"},{"location":"formats/object-detection/yolov9/#pattern-1-images-and-labels-as-root-directories","title":"Pattern 1: Images and Labels as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 image3.jpg\n\u2502       \u2514\u2500\u2500 image4.jpg\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 image1.txt\n    \u2502   \u2514\u2500\u2500 image2.txt\n    \u2514\u2500\u2500 val/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre>"},{"location":"formats/object-detection/yolov9/#pattern-2-trainval-as-root-directories","title":"Pattern 2: Train/Val as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image3.jpg\n    \u2502   \u2514\u2500\u2500 image4.jpg\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre> <p>The corresponding <code>data.yaml</code> configuration should match your chosen structure:</p> <pre><code># For Pattern 1:\npath: .  # Optional - defaults to current directory if omitted\ntrain: images/train  # Path to training images\nval: images/val      # Path to validation images\n\n# For Pattern 2:\npath: .  # Optional - defaults to current directory if omitted\ntrain: train/images  # Path to training images\nval: val/images      # Path to validation images\n</code></pre> <p>Important: Label files must have the same name as their corresponding image files (excluding the file extension) and must maintain the parallel directory structure, only replacing <code>images</code> with <code>labels</code> in the path. </p>"},{"location":"formats/object-detection/yolov9/#benefits-of-yolov9-format","title":"Benefits of YOLOv9 Format","text":"<ul> <li>Simplicity: Easy to read and write, facilitating quick dataset preparation.</li> <li>Efficiency: Compact representation reduces storage requirements.</li> <li>Compatibility: Maintains consistency across YOLO versions, ensuring seamless integration with various tools and frameworks.</li> </ul>"},{"location":"formats/object-detection/yolov9/#example-of-yolov9-format","title":"Example of YOLOv9 Format","text":"<p>Example of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory (defaults to current directory if omitted). Can also be `../dataset`.\ntrain: images/train  # Directory for training images\nval: images/val  # Directory for validation images\ntest: images/test  # Directory for test images (optional)\n\nnames:\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Example Annotation</p> <p>For an image named <code>image1.jpg</code>, the corresponding <code>image1.txt</code> might contain: <pre><code>0 0.716797 0.395833 0.216406 0.147222\n1 0.687500 0.379167 0.255208 0.175000\n</code></pre></p> <p>Explanation: - The first line represents an object of class <code>0</code> (e.g., <code>cat</code>) with its bounding box centered at <code>(0.716797, 0.395833)</code> relative to the image dimensions, and a width and height of <code>0.216406</code> and <code>0.147222</code> respectively. - The second line represents an object of class <code>1</code> (e.g., <code>dog</code>) with its own bounding box specifications.</p>"},{"location":"formats/object-detection/yolov9/#normalizing-bounding-box-coordinates-for-yolov9","title":"Normalizing Bounding Box Coordinates for YOLOv9","text":"<p>To convert pixel values to normalized values required by YOLOv9:</p> <pre><code># Given pixel values and image dimensions\nx_top_left = 150     # x coordinate of top-left corner of bounding box\ny_top_left = 200     # y coordinate of top-left corner of bounding box\nwidth_pixel = 50     # width of bounding box\nheight_pixel = 80    # height of bounding box\nimage_width = 640\nimage_height = 480\n\n# 1. Convert top-left coordinates to center coordinates (in pixels)\nx_center_pixel = x_top_left + (width_pixel / 2)   # 150 + (50/2) = 175\ny_center_pixel = y_top_left + (height_pixel / 2)  # 200 + (80/2) = 240\n\n# 2. Normalize all values (divide by image dimensions)\nx_center = x_center_pixel / image_width     # 175 / 640 = 0.273438\ny_center = y_center_pixel / image_height    # 240 / 480 = 0.500000\nwidth = width_pixel / image_width           # 50 / 640 = 0.078125\nheight = height_pixel / image_height        # 80 / 480 = 0.166667\n\n# Annotation line format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\nannotation = f\"0 {x_center} {y_center} {width} {height}\"\n# Output: \"0 0.273438 0.500000 0.078125 0.166667\"\n</code></pre>"},{"location":"formats/object-detection/yolov9/#converting-annotations-to-yolov9-format-with-labelformat","title":"Converting Annotations to YOLOv9 Format with Labelformat","text":"<p>Our Labelformat framework simplifies the process of converting various annotation formats to the YOLOv9 detection format. Below is a step-by-step guide to perform this conversion.</p>"},{"location":"formats/object-detection/yolov9/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed. You can install it via pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/yolov9/#conversion-example-coco-to-yolov9","title":"Conversion Example: COCO to YOLOv9","text":"<p>Assume you have annotations in the COCO format and wish to convert them to YOLOv9. Here\u2019s how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard COCO structure:</p> <ul> <li>You have a <code>.json</code> file with the COCO annotations. (e.g. <code>annotations/instances_train.json</code>)</li> <li>You have a directory with the images. (e.g. <code>images/</code>)</li> </ul> <p>Full example: <pre><code>dataset/\n\u251c\u2500\u2500 annotations/\n\u2502   \u2514\u2500\u2500 instances_train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n</code></pre></p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert COCO annotations to YOLOv9: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov9 \\\n    --output-folder dataset/yolov9_labels \\\n    --output-split train\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 yolov9_labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n</code></pre></p> <p>Contents of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory\ntrain: images  # Directory for training images\nnc: 3  # Number of classes\nnames:  # Class name mapping\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Contents of <code>image1.txt</code>: <pre><code>0 0.234375 0.416667 0.078125 0.166667\n1 0.500000 0.500000 0.100000 0.200000\n</code></pre></p>"},{"location":"formats/object-detection/yolov9/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>The format implementation includes several safeguards:</p> <ul> <li>Missing Label Files:</li> <li>Warning is logged if a label file doesn't exist for an image</li> <li> <p>Image is skipped from processing</p> </li> <li> <p>File Access Issues:</p> </li> <li>Errors are logged if label files cannot be read due to permissions or other OS issues</li> <li> <p>Affected images are skipped from processing</p> </li> <li> <p>Label Format Validation:</p> </li> <li>Each line must contain exactly 5 space-separated values</li> <li>Invalid lines are logged as warnings and skipped</li> <li>All values must be convertible to appropriate types:<ul> <li>Category ID must be a valid integer</li> <li>Coordinates and dimensions must be valid floats</li> </ul> </li> <li>Category IDs must exist in the category mapping</li> </ul> <p>Example of a properly formatted label file: <pre><code>0 0.716797 0.395833 0.216406 0.147222  # Each line must have exactly 5 space-separated values\n1 0.687500 0.379167 0.255208 0.175000  # All values must be valid numbers within [0,1] range\n</code></pre></p>"},{"location":"tutorials/converting-coco-to-yolov8/","title":"Converting COCO Labels to YOLOv8 Format","text":"<p>This tutorial walks you through converting object detection labels from the COCO format to the YOLOv8 format using Labelformat's CLI and Python API.</p>"},{"location":"tutorials/converting-coco-to-yolov8/#prerequisites","title":"Prerequisites","text":"<ul> <li>Labelformat Installed: Follow the Installation Guide.</li> <li>COCO Dataset: Ensure you have a COCO-formatted dataset ready for conversion.</li> </ul>"},{"location":"tutorials/converting-coco-to-yolov8/#step-1-prepare-your-dataset","title":"Step 1: Prepare Your Dataset","text":"<p>Organize your dataset with the following structure:</p> <pre><code>project/\n\u251c\u2500\u2500 coco-labels/\n\u2502   \u2514\u2500\u2500 train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2514\u2500\u2500 image2.jpg\n</code></pre> <p>Ensure that <code>train.json</code> contains the COCO annotations and that all images are located in the <code>images/</code> directory.</p>"},{"location":"tutorials/converting-coco-to-yolov8/#step-2-using-the-cli-for-conversion","title":"Step 2: Using the CLI for Conversion","text":"<p>Open your terminal and navigate to the <code>project/</code> directory.</p> <p>Run the following command to convert COCO labels to YOLOv8:</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file coco-labels/train.json \\\n    --output-format yolov8 \\\n    --output-file yolo-labels/data.yaml \\\n    --output-split train\n</code></pre>"},{"location":"tutorials/converting-coco-to-yolov8/#explanation-of-the-command","title":"Explanation of the Command:","text":"<ul> <li><code>--task object-detection</code>: Specifies the task type.</li> <li><code>--input-format coco</code>: Defines the input label format.</li> <li><code>--input-file coco-labels/train.json</code>: Path to the COCO annotations file.</li> <li><code>--output-format yolov8</code>: Desired output format.</li> <li><code>--output-file yolo-labels/data.yaml</code>: Path to save the YOLOv8 configuration file.</li> <li><code>--output-split train</code>: Data split label.</li> </ul>"},{"location":"tutorials/converting-coco-to-yolov8/#step-3-verify-the-conversion","title":"Step 3: Verify the Conversion","text":"<p>After running the command, your project structure should include:</p> <pre><code>project/\n\u251c\u2500\u2500 yolo-labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n</code></pre> <ul> <li><code>data.yaml</code>: YOLOv8 configuration file containing category names and paths.</li> <li><code>labels/</code>: Directory containing YOLOv8-formatted label files.</li> </ul> <p>Sample <code>data.yaml</code>: <pre><code>names:\n  0: cat\n  1: dog\n  2: fish\nnc: 3\npath: .\ntrain: images\n</code></pre></p> <p>Sample Label File (<code>image1.txt</code>):</p> <pre><code>2 0.8617 0.7308 0.0359 0.0433\n0 0.8180 0.6911 0.0328 0.0793\n</code></pre> <ul> <li>Format: <code>&lt;category_id&gt; &lt;center_x&gt; &lt;center_y&gt; &lt;width&gt; &lt;height&gt;</code></li> <li>Coordinates: Normalized between 0 and 1.</li> </ul>"},{"location":"tutorials/converting-coco-to-yolov8/#step-4-using-the-python-api-for-conversion","title":"Step 4: Using the Python API for Conversion","text":"<p>If you prefer using Python for more control, follow these steps.</p>"},{"location":"tutorials/converting-coco-to-yolov8/#41-write-the-conversion-script","title":"4.1: Write the Conversion Script","text":"<p>Create a Python script named <code>coco_to_yolov8.py</code> with the following content:</p> <pre><code>from pathlib import Path\nfrom labelformat.formats import COCOObjectDetectionInput, YOLOv8ObjectDetectionOutput\n\n# Define input and output paths\ncoco_input_path = Path(\"coco-labels/train.json\")\nyolo_output_path = Path(\"yolo-labels/data.yaml\")\n\n# Initialize input and output classes\ncoco_input = COCOObjectDetectionInput(input_file=coco_input_path)\nyolo_output = YOLOv8ObjectDetectionOutput(\n    output_file=yolo_output_path,\n    output_split=\"train\"\n)\n\n# Perform the conversion\nyolo_output.save(label_input=coco_input)\n\nprint(\"Conversion from COCO to YOLOv8 completed successfully!\")\n</code></pre>"},{"location":"tutorials/converting-coco-to-yolov8/#42-execute-the-script","title":"4.2: Execute the Script","text":"<p>Run the script using Python:</p> <pre><code>python coco_to_yolov8.py\n</code></pre> <p>Upon successful execution, you will see:</p> <pre><code>Conversion from COCO to YOLOv8 completed successfully!\n</code></pre>"},{"location":"tutorials/converting-coco-to-yolov8/#step-5-integrate-with-your-training-pipeline","title":"Step 5: Integrate with Your Training Pipeline","text":"<p>Use the generated YOLOv8 labels (<code>data.yaml</code> and <code>labels/</code> directory) to train your YOLOv8 models seamlessly.</p> <p>Example YOLOv8 Training Command:</p> <pre><code>yolo detect train data=yolo-labels/data.yaml model=yolov8s.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tutorials/converting-coco-to-yolov8/#conclusion","title":"Conclusion","text":"<p>You've successfully converted COCO labels to YOLOv8 format using both the CLI and Python API. Labelformat simplifies label format conversions, enabling efficient integration into your computer vision projects.</p>"},{"location":"tutorials/converting-coco-to-yolov8/#next-steps","title":"Next Steps","text":"<ul> <li>Explore other supported 2d object detection formats in the Supported Object Detection Formats section.</li> </ul>"}]}